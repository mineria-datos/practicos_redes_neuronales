[
["index.html", "Prácticos Redes Neuronales Guía 1 Guía 1 1.1 Ejercicio 1 1.2 Ejercicio 2 1.3 Ejercicio 3 1.4 Ejercicio 4", " Prácticos Redes Neuronales Grupo 3: Emiliano Bodean - Zacarias Ojeda 2020-02-03 Guía 1 Guía 1 1.1 Ejercicio 1 Realice un programa que permita el entrenamiento y prueba de un perceptrón simple con una cantidad variable de entradas. 1.1.1 Resolución del problema OR Lectura de los patrones de entrenamiento OR_trn &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/OR_trn.csv&quot;, col_names = FALSE) OR_tst &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/OR_tst.csv&quot;, col_names = FALSE) Selección de parámetros y entrenamiento de perceptrón Utilizamos las funciones implementadas en el archivo “PerceptronSimple.R” if (calcular) { salidaPerceptronOR &lt;- entrenarPerceptron(OR_trn, maxEpocas = 10, critFinalizacion = 0.9) } # Modelo obtenido salidaPerceptronOR ## $W ## [1] 1.4500000 0.6356451 0.8985932 ## ## $error ## [1] 0.018 ## ## $epoca ## [1] 1 ## ## $tasa ## [1] 1 El entrenamiento nos devuelve la tasa obtenida en cada época y al final mostramos los valores de los pesos w que definen la recta del modelo. $ pesos = [w_0, w_1, w_2, …w_n] $ Graficas graficarRectaSeparacion(salidaPerceptronOR$W, OR_trn) En la gráfica vemos como la recta del modelo separa el dominio según la clase. Prueba con datos de test test &lt;- aplicarPerceptron(salidaPerceptronOR$W, OR_tst) test$tasa ## [1] 1 Por ser este un problema sencillo donde las clases son separables por una recta, la tasa de aciertos en test nos da un 100%. 1.1.2 Resolución del problema XOR Lectura de los patrones de entrenamiento XOR_trn &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/XOR_trn.csv&quot;, col_names = FALSE) XOR_tst &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/XOR_tst.csv&quot;, col_names = FALSE) Selección de parámetros y entrenamiento de perceptrón if (calcular) { salidaPerceptronXOR &lt;- entrenarPerceptron(XOR_trn, maxEpocas = 10, critFinalizacion = 0.8) } salidaPerceptronXOR ## $W ## [1] 504.500000 -3.568637 -3.404218 ## ## $error ## [1] 2.014 2.014 2.014 2.014 2.014 2.014 2.014 2.014 2.014 2.014 ## ## $epoca ## [1] 10 ## ## $tasa ## [1] 0.4965 Vemos que luego de 10 épocas el perceptrón simple no logra resolver el problema del XOR, ni tampoco mejorar su tasa de aciertos. Graficas graficarRectaSeparacion(salidaPerceptronXOR$W, XOR_trn) En la gráfica vemos la recta obtenida que deja todos los patrones clasificados de igual manera. Prueba con datos de test test &lt;- aplicarPerceptron(salidaPerceptronXOR$W, XOR_tst) test$tasa ## [1] 0.44 Este problema no pudo ser resuelto por le perceptrón simple, la tasa de aciertos obtenida es peor que el azar. 1.1.3 Preguntas: ¿Pueden ser resueltos ambos problemas (OR y XOR) empleando un perceptrón simple? Justifique su respuesta. En los resultados obtenidos verificamos que no se puede resolver el problema XOR con el perceptrón simple porque no es posible dividir la clase con una recta que separe el dominio en dos. ¿Qué efecto tiene la tasa de aprendizaje en el entrenamiento del perceptrón? Explique cómo este parámetro afecta a la actualización de la frontera de decisión La tasa de aprendizaje nos permite variar cuanto se actualizan los valores de los pesos. Una tasa muy baja puede generar que el algoritmo demore mucho en llegar al mínimo, y una tasa muy alta puede generar que el algoritmo diverja y nunca llegue a un mínimo. En nuestro caso, se implementó una función que posee una tasa de aprendizaje de 0,05 (nu=0.05) por defecto y no fue necesario cambiarlo. 1.2 Ejercicio 2 1.2.1 Punto a Lectura de datos spheres1d10 &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/spheres1d10.csv&quot;, col_names = FALSE) #Nombre de las columnas colnames(spheres1d10) ## [1] &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; #Dimenciones de los datos dim(spheres1d10) ## [1] 1000 4 Grafica plot_ly(x=spheres1d10$X1, y=spheres1d10$X2, z=spheres1d10$X3, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color=spheres1d10$X4) Generamos las particiones Utilizamos la función implementada en el archivo “Particiones.R” particion_spheres1d10 &lt;- generarNParticionesPorID(dataset = spheres1d10, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 1, clase = &quot;X4&quot;) Esta función nos devuelve un listado de 5 listas de ID para entrenamiento, de aproximadamente 800 elementos, y 5 listas de ID para prueba, de aproximadamente 200 elementos. Generamos los modelos con el perceptrón simple. for (n in seq(1,5)) { print(glue::glue(&quot;Partición: {n}&quot;)) #Entrenamiento salidaPerceptron &lt;- entrenarPerceptronSigmo(spheres1d10[particion_spheres1d10$trn[[n]],], maxEpocas = 20, critFinalizacion = 0.8) print(glue::glue(&quot;Pesos: {salidaPerceptron$W}&quot;)) #Prueba test &lt;- aplicarPerceptron(salidaPerceptron$W, spheres1d10[particion_spheres1d10$tst[[n]],]) print(glue::glue(&quot;Tasa de aciertos en test: {test$tasa}&quot;)) } ## Partición: 1 ## Epoca: 1 - Tasa: 0.9025 - Error: 0.665257488776491 ## Pesos: -0.803701945156201 ## Pesos: -1.87904593332309 ## Pesos: 0.604169404954752 ## Pesos: -0.754999128250459 ## Tasa de aciertos en test: 0.65 ## Partición: 2 ## Epoca: 1 - Tasa: 0.90625 - Error: 0.646385898190898 ## Pesos: -0.792322956907711 ## Pesos: -1.95992164139631 ## Pesos: 0.672281659055264 ## Pesos: -0.753572812822427 ## Tasa de aciertos en test: 0.665 ## Partición: 3 ## Epoca: 1 - Tasa: 0.90625 - Error: 0.638697222751195 ## Pesos: -0.788286465415768 ## Pesos: -2.06493132737021 ## Pesos: 0.701554564832055 ## Pesos: -0.775721279617205 ## Tasa de aciertos en test: 0.65 ## Partición: 4 ## Epoca: 1 - Tasa: 0.790262172284644 - Error: 0.696308631575471 ## Epoca: 2 - Tasa: 0.897627965043695 - Error: 0.552779013636828 ## Pesos: -1.15059992006629 ## Pesos: -2.5724849000268 ## Pesos: 1.04038949775025 ## Pesos: -1.1521801945307 ## Tasa de aciertos en test: 0.648241206030151 ## Partición: 5 ## Epoca: 1 - Tasa: 0.818523153942428 - Error: 0.669037300819362 ## Pesos: -0.661068660757953 ## Pesos: -1.80961606107043 ## Pesos: 0.518801177895036 ## Pesos: -0.597477336998598 ## Tasa de aciertos en test: 0.746268656716418 1.2.2 Punto b Lectura de datos spheres2d10 &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/spheres2d10.csv&quot;, col_names = FALSE) spheres2d50 &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/spheres2d50.csv&quot;, col_names = FALSE) spheres2d70 &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/spheres2d70.csv&quot;, col_names = FALSE) Generamos las particiones Llamamos a la función dos veces con dos semillas para obtener las 10 particiones. La función implementada genera las particiones sin reemplazo, con una relación 80/20 no se pueden generar más de 5 particiones. # 10% particion_spheres2d10 &lt;- generarNParticionesPorID(dataset = spheres2d10, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 1, clase = &quot;X4&quot;) particion_spheres2d10$trn &lt;- c(particion_spheres2d10$trn, generarNParticionesPorID( dataset = spheres2d10, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 12, clase = &quot;X4&quot;)$trn) particion_spheres2d10$tst &lt;- c(particion_spheres2d10$tst, generarNParticionesPorID( dataset = spheres2d10, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 12, clase = &quot;X4&quot;)$tst) # 50% particion_spheres2d50 &lt;- generarNParticionesPorID(dataset = spheres2d50, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 1, clase = &quot;X4&quot;) particion_spheres2d50$trn &lt;- c(particion_spheres2d50$trn, generarNParticionesPorID( dataset = spheres2d50, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 12, clase = &quot;X4&quot;)$trn) particion_spheres2d50$tst &lt;- c(particion_spheres2d50$tst, generarNParticionesPorID( dataset = spheres2d50, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 12, clase = &quot;X4&quot;)$tst) # 70% particion_spheres2d70 &lt;- generarNParticionesPorID(dataset = spheres2d70, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 1, clase = &quot;X4&quot;) particion_spheres2d70$trn &lt;- c(particion_spheres2d70$trn, generarNParticionesPorID( dataset = spheres2d70, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 12, clase = &quot;X4&quot;)$trn) particion_spheres2d70$tst &lt;- c(particion_spheres2d70$tst, generarNParticionesPorID( dataset = spheres2d70, nroParticiones = 5, porcEntrenamiento = 0.8, semilla = 12, clase = &quot;X4&quot;)$tst) Generamos los modelos con el perceptrón simple. – Datos con desviaciones del 10% plot_ly(x=spheres2d10$X1, y=spheres2d10$X2, z=spheres2d10$X3, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color=spheres2d10$X4) tasaMedia &lt;- 0 for (n in seq(1,10)) { print(glue::glue(&quot;Partición: {n}&quot;)) #Entrenamiento salidaPerceptron &lt;- entrenarPerceptron(spheres2d10[particion_spheres2d10$trn[[n]],], maxEpocas = 10, critFinalizacion = 0.79) print(glue::glue(&quot;Pesos: {salidaPerceptron$W}&quot;)) #Pruba test &lt;- aplicarPerceptron(salidaPerceptron$W, spheres2d10[particion_spheres2d10$tst[[n]],]) print(glue::glue(&quot;Tasa de aciertos en test: {test$tasa}&quot;)) tasaMedia &lt;- tasaMedia + test$tasa } ## Partición: 1 ## Epoca: 1 - Tasa: 0.769 - Error: 0.989 ## Epoca: 2 - Tasa: 0.782 - Error: 0.892 ## Epoca: 3 - Tasa: 0.78375 - Error: 0.867 ## Epoca: 4 - Tasa: 0.786 - Error: 0.857 ## Epoca: 5 - Tasa: 0.7875 - Error: 0.853 ## Epoca: 6 - Tasa: 0.7885 - Error: 0.847 ## Epoca: 7 - Tasa: 0.789 - Error: 0.845 ## Epoca: 8 - Tasa: 0.78925 - Error: 0.843 ## Epoca: 9 - Tasa: 0.79 - Error: 0.842 ## Epoca: 10 - Tasa: 0.78975 - Error: 0.842 ## Pesos: 430.450000000064 ## Pesos: -424.187208931031 ## Pesos: 13.8755067429983 ## Pesos: -14.2785229928842 ## Tasa de aciertos en test: 0.808 ## Partición: 2 ## Epoca: 1 - Tasa: 0.77275 - Error: 0.966 ## Epoca: 2 - Tasa: 0.7835 - Error: 0.881 ## Epoca: 3 - Tasa: 0.78875 - Error: 0.854 ## Epoca: 4 - Tasa: 0.7905 - Error: 0.844 ## Pesos: 178.050000000006 ## Pesos: -175.075226431031 ## Pesos: 5.66492174299819 ## Pesos: -6.04556849288417 ## Tasa de aciertos en test: 0.778 ## Partición: 3 ## Epoca: 1 - Tasa: 0.77725 - Error: 0.979 ## Epoca: 2 - Tasa: 0.78625 - Error: 0.884 ## Epoca: 3 - Tasa: 0.7895 - Error: 0.864 ## Epoca: 4 - Tasa: 0.79 - Error: 0.843 ## Epoca: 5 - Tasa: 0.7915 - Error: 0.837 ## Pesos: 220.350000000016 ## Pesos: -216.714430931031 ## Pesos: 7.62960274299818 ## Pesos: -7.15906199288417 ## Tasa de aciertos en test: 0.788 ## Partición: 4 ## Epoca: 1 - Tasa: 0.771307173206698 - Error: 0.978755311172207 ## Epoca: 2 - Tasa: 0.782304423894027 - Error: 0.887778055486128 ## Epoca: 3 - Tasa: 0.78530367408148 - Error: 0.865783554111472 ## Epoca: 4 - Tasa: 0.787053236690827 - Error: 0.852786803299175 ## Epoca: 5 - Tasa: 0.787803049237691 - Error: 0.848787803049238 ## Epoca: 6 - Tasa: 0.789802549362659 - Error: 0.8447888027993 ## Epoca: 7 - Tasa: 0.790552361909523 - Error: 0.841789552611847 ## Pesos: 305.200000000035 ## Pesos: -300.520149931031 ## Pesos: 10.0954057429982 ## Pesos: -10.4414904928842 ## Tasa de aciertos en test: 0.796796796796797 ## Partición: 5 ## Epoca: 1 - Tasa: 0.77344336084021 - Error: 0.974243560890223 ## Epoca: 2 - Tasa: 0.783195798949737 - Error: 0.876219054763691 ## Epoca: 3 - Tasa: 0.787696924231058 - Error: 0.8512128032008 ## Epoca: 4 - Tasa: 0.791447861965491 - Error: 0.841210302575644 ## Pesos: 177.400000000006 ## Pesos: -174.468028431031 ## Pesos: 6.17010574299817 ## Pesos: -5.88406849288417 ## Tasa de aciertos en test: 0.787212787212787 ## Partición: 6 ## Epoca: 1 - Tasa: 0.773 - Error: 0.978 ## Epoca: 2 - Tasa: 0.784 - Error: 0.875 ## Epoca: 3 - Tasa: 0.78775 - Error: 0.858 ## Epoca: 4 - Tasa: 0.78975 - Error: 0.849 ## Epoca: 5 - Tasa: 0.7915 - Error: 0.836 ## Pesos: 219.700000000016 ## Pesos: -216.098115431031 ## Pesos: 6.95935524299818 ## Pesos: -7.47617049288416 ## Tasa de aciertos en test: 0.777 ## Partición: 7 ## Epoca: 1 - Tasa: 0.776 - Error: 0.974 ## Epoca: 2 - Tasa: 0.7855 - Error: 0.874 ## Epoca: 3 - Tasa: 0.7895 - Error: 0.847 ## Epoca: 4 - Tasa: 0.793 - Error: 0.833 ## Pesos: 176.800000000006 ## Pesos: -173.952994931031 ## Pesos: 5.66795124299818 ## Pesos: -6.24085349288418 ## Tasa de aciertos en test: 0.786 ## Partición: 8 ## Epoca: 1 - Tasa: 0.7765 - Error: 0.963 ## Epoca: 2 - Tasa: 0.78625 - Error: 0.869 ## Epoca: 3 - Tasa: 0.7905 - Error: 0.845 ## Pesos: 134.849999999996 ## Pesos: -132.368980931032 ## Pesos: 4.87620924299819 ## Pesos: -4.43657649288418 ## Tasa de aciertos en test: 0.777 ## Partición: 9 ## Epoca: 1 - Tasa: 0.769057735566108 - Error: 0.976755811047238 ## Epoca: 2 - Tasa: 0.779555111222194 - Error: 0.893776555861035 ## Epoca: 3 - Tasa: 0.783304173956511 - Error: 0.862784303924019 ## Epoca: 4 - Tasa: 0.785053736565859 - Error: 0.852786803299175 ## Epoca: 5 - Tasa: 0.786303424143964 - Error: 0.849787553111722 ## Epoca: 6 - Tasa: 0.787553111722069 - Error: 0.847788052986753 ## Epoca: 7 - Tasa: 0.788302924268933 - Error: 0.845788552861785 ## Epoca: 8 - Tasa: 0.789802549362659 - Error: 0.841789552611847 ## Epoca: 9 - Tasa: 0.789552611847038 - Error: 0.841789552611847 ## Epoca: 10 - Tasa: 0.790302424393902 - Error: 0.838790302424394 ## Pesos: 430.500000000064 ## Pesos: -423.970796931032 ## Pesos: 14.2361747429982 ## Pesos: -14.2684639928842 ## Tasa de aciertos en test: 0.7997997997998 ## Partición: 10 ## Epoca: 1 - Tasa: 0.769692423105776 - Error: 0.990247561890473 ## Epoca: 2 - Tasa: 0.778944736184046 - Error: 0.895223805951488 ## Epoca: 3 - Tasa: 0.783695923980995 - Error: 0.868217054263566 ## Epoca: 4 - Tasa: 0.787946986746687 - Error: 0.857214303575894 ## Epoca: 5 - Tasa: 0.788947236809202 - Error: 0.849212303075769 ## Epoca: 6 - Tasa: 0.78944736184046 - Error: 0.844211052763191 ## Epoca: 7 - Tasa: 0.790197549387347 - Error: 0.845211302825706 ## Pesos: 306.500000000035 ## Pesos: -301.852386431031 ## Pesos: 10.4061822429982 ## Pesos: -9.8053649928842 ## Tasa de aciertos en test: 0.805194805194805 print(glue::glue(&quot;Tasa de aciertos media en test: {tasaMedia/10}&quot;)) ## Tasa de aciertos media en test: 0.790300418900419 – Datos con desviaciones del 50% plot_ly(x=spheres2d50$X1, y=spheres2d50$X2, z=spheres2d50$X3, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color=spheres2d50$X4) tasaMedia &lt;- 0 for (n in seq(1,10)) { print(glue::glue(&quot;Partición: {n}&quot;)) #Entrenamiento salidaPerceptron &lt;- entrenarPerceptron(spheres2d50[particion_spheres2d50$trn[[n]],], maxEpocas = 5, critFinalizacion = 0.79) print(glue::glue(&quot;Pesos: {salidaPerceptron$W}&quot;)) #Pruba test &lt;- aplicarPerceptron(salidaPerceptron$W, spheres2d50[particion_spheres2d50$tst[[n]],]) print(glue::glue(&quot;Tasa de aciertos en test: {test$tasa}&quot;)) tasaMedia &lt;- tasaMedia + test$tasa } ## Partición: 1 ## Epoca: 1 - Tasa: 0.78975 - Error: 0.885 ## Epoca: 2 - Tasa: 0.79 - Error: 0.825 ## Epoca: 3 - Tasa: 0.7905 - Error: 0.827 ## Pesos: 125.449999999995 ## Pesos: -115.659149431031 ## Pesos: 10.7080272429982 ## Pesos: -12.4834539928842 ## Tasa de aciertos en test: 0.776 ## Partición: 2 ## Epoca: 1 - Tasa: 0.788302924268933 - Error: 0.897775556110972 ## Epoca: 2 - Tasa: 0.791552111972007 - Error: 0.832791802049488 ## Pesos: 85.5499999999973 ## Pesos: -78.6916419310314 ## Pesos: 7.12469924299819 ## Pesos: -9.32485249288417 ## Tasa de aciertos en test: 0.78978978978979 ## Partición: 3 ## Epoca: 1 - Tasa: 0.7871967991998 - Error: 0.893223305826457 ## Epoca: 2 - Tasa: 0.791197799449862 - Error: 0.837209302325581 ## Pesos: 85.4999999999973 ## Pesos: -78.6647159310315 ## Pesos: 7.56831824299819 ## Pesos: -9.52412449288418 ## Tasa de aciertos en test: 0.792207792207792 ## Partición: 4 ## Epoca: 1 - Tasa: 0.783554111472132 - Error: 0.888777805548613 ## Epoca: 2 - Tasa: 0.78530367408148 - Error: 0.8447888027993 ## Epoca: 3 - Tasa: 0.786803299175206 - Error: 0.8447888027993 ## Epoca: 4 - Tasa: 0.786803299175206 - Error: 0.841789552611847 ## Epoca: 5 - Tasa: 0.787053236690827 - Error: 0.841789552611847 ## Pesos: 209.150000000013 ## Pesos: -192.404869431031 ## Pesos: 18.1550392429982 ## Pesos: -21.0932719928842 ## Tasa de aciertos en test: 0.787787787787788 ## Partición: 5 ## Epoca: 1 - Tasa: 0.785196299074769 - Error: 0.905226306576644 ## Epoca: 2 - Tasa: 0.787446861715429 - Error: 0.856214053513378 ## Epoca: 3 - Tasa: 0.787946986746687 - Error: 0.853213303325831 ## Epoca: 4 - Tasa: 0.787946986746687 - Error: 0.853213303325831 ## Epoca: 5 - Tasa: 0.787696924231058 - Error: 0.852213053263316 ## Pesos: 210.550000000014 ## Pesos: -194.378791431032 ## Pesos: 17.2403147429982 ## Pesos: -21.8316539928842 ## Tasa de aciertos en test: 0.804195804195804 ## Partición: 6 ## Epoca: 1 - Tasa: 0.7825 - Error: 0.909 ## Epoca: 2 - Tasa: 0.785 - Error: 0.862 ## Epoca: 3 - Tasa: 0.7855 - Error: 0.855 ## Epoca: 4 - Tasa: 0.78625 - Error: 0.853 ## Epoca: 5 - Tasa: 0.78725 - Error: 0.851 ## Pesos: 211.900000000014 ## Pesos: -195.984205931031 ## Pesos: 18.2026532429982 ## Pesos: -20.7345039928842 ## Tasa de aciertos en test: 0.802 ## Partición: 7 ## Epoca: 1 - Tasa: 0.786053486628343 - Error: 0.900774806298425 ## Epoca: 2 - Tasa: 0.788802799300175 - Error: 0.846788302924269 ## Epoca: 3 - Tasa: 0.790552361909523 - Error: 0.840789802549363 ## Pesos: 127.949999999995 ## Pesos: -117.771433931031 ## Pesos: 10.9535267429982 ## Pesos: -13.5674044928842 ## Tasa de aciertos en test: 0.791791791791792 ## Partición: 8 ## Epoca: 1 - Tasa: 0.78944736184046 - Error: 0.888222055513878 ## Epoca: 2 - Tasa: 0.791197799449862 - Error: 0.830207551887972 ## Pesos: 84.8999999999973 ## Pesos: -78.0497004310314 ## Pesos: 7.16728624299819 ## Pesos: -9.47793799288418 ## Tasa de aciertos en test: 0.79020979020979 ## Partición: 9 ## Epoca: 1 - Tasa: 0.787053236690827 - Error: 0.886778305423644 ## Epoca: 2 - Tasa: 0.788802799300175 - Error: 0.835791052236941 ## Epoca: 3 - Tasa: 0.789802549362659 - Error: 0.834791302174456 ## Epoca: 4 - Tasa: 0.791052236940765 - Error: 0.835791052236941 ## Pesos: 165.600000000003 ## Pesos: -152.597184931031 ## Pesos: 14.3373507429982 ## Pesos: -16.9819009928842 ## Tasa de aciertos en test: 0.783783783783784 ## Partición: 10 ## Epoca: 1 - Tasa: 0.788447111777944 - Error: 0.89022255563891 ## Epoca: 2 - Tasa: 0.79169792448112 - Error: 0.837209302325581 ## Pesos: 85.9499999999973 ## Pesos: -79.0637449310314 ## Pesos: 7.03330824299819 ## Pesos: -9.31209399288416 ## Tasa de aciertos en test: 0.79020979020979 print(glue::glue(&quot;Tasa de aciertos media en test: {tasaMedia/10}&quot;)) ## Tasa de aciertos media en test: 0.790797632997633 – Datos con desviaciones del 70% plot_ly(x=spheres2d70$X1, y=spheres2d70$X2, z=spheres2d70$X3, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, color=spheres2d70$X4) tasaMedia &lt;- 0 for (n in seq(1,10)) { print(glue::glue(&quot;Partición: {n}&quot;)) #Entrenamiento salidaPerceptron &lt;- entrenarPerceptron(spheres2d70[particion_spheres2d70$trn[[n]],], maxEpocas = 10, critFinalizacion = 0.77) print(glue::glue(&quot;Pesos: {salidaPerceptron$W}&quot;)) #Pruba test &lt;- aplicarPerceptron(salidaPerceptron$W, spheres2d70[particion_spheres2d70$tst[[n]],]) print(glue::glue(&quot;Tasa de aciertos en test: {test$tasa}&quot;)) tasaMedia &lt;- tasaMedia + test$tasa } ## Partición: 1 ## Epoca: 1 - Tasa: 0.77 - Error: 0.959 ## Epoca: 2 - Tasa: 0.77025 - Error: 0.931 ## Pesos: 89.999999999997 ## Pesos: -79.8077959310314 ## Pesos: 9.90327824299819 ## Pesos: -12.2228474928842 ## Tasa de aciertos en test: 0.775 ## Partición: 2 ## Epoca: 1 - Tasa: 0.77405648587853 - Error: 0.952761809547613 ## Pesos: 46.5499999999995 ## Pesos: -41.1042084310315 ## Pesos: 5.05819474299818 ## Pesos: -6.77470349288419 ## Tasa de aciertos en test: 0.764764764764765 ## Partición: 3 ## Epoca: 1 - Tasa: 0.773943485871468 - Error: 0.946236559139785 ## Pesos: 46.4999999999995 ## Pesos: -41.1782639310315 ## Pesos: 4.49773974299818 ## Pesos: -6.99084399288419 ## Tasa de aciertos en test: 0.769230769230769 ## Partición: 4 ## Epoca: 1 - Tasa: 0.771057235691077 - Error: 0.969757560609848 ## Pesos: 47.4999999999994 ## Pesos: -42.0557839310315 ## Pesos: 4.89473724299818 ## Pesos: -7.19432899288419 ## Tasa de aciertos en test: 0.786786786786787 ## Partición: 5 ## Epoca: 1 - Tasa: 0.77569392348087 - Error: 0.949237309327332 ## Pesos: 46.5499999999995 ## Pesos: -41.0684964310315 ## Pesos: 4.83141774299818 ## Pesos: -7.05025849288419 ## Tasa de aciertos en test: 0.767232767232767 ## Partición: 6 ## Epoca: 1 - Tasa: 0.773 - Error: 0.96 ## Pesos: 46.8999999999995 ## Pesos: -41.6534594310315 ## Pesos: 4.62442724299818 ## Pesos: -6.89817399288419 ## Tasa de aciertos en test: 0.773 ## Partición: 7 ## Epoca: 1 - Tasa: 0.773806548362909 - Error: 0.948762809297676 ## Pesos: 46.6499999999995 ## Pesos: -41.1666364310315 ## Pesos: 5.00322424299818 ## Pesos: -6.93943999288419 ## Tasa de aciertos en test: 0.766766766766767 ## Partición: 8 ## Epoca: 1 - Tasa: 0.765191297824456 - Error: 0.977244311077769 ## Epoca: 2 - Tasa: 0.765191297824456 - Error: 0.942235558889722 ## Epoca: 3 - Tasa: 0.764691172793198 - Error: 0.940235058764691 ## Epoca: 4 - Tasa: 0.765191297824456 - Error: 0.944236059014754 ## Epoca: 5 - Tasa: 0.764941235308827 - Error: 0.946236559139785 ## Epoca: 6 - Tasa: 0.764941235308827 - Error: 0.945236309077269 ## Epoca: 7 - Tasa: 0.764941235308827 - Error: 0.945236309077269 ## Epoca: 8 - Tasa: 0.764691172793198 - Error: 0.945236309077269 ## Epoca: 9 - Tasa: 0.764441110277569 - Error: 0.943235808952238 ## Epoca: 10 - Tasa: 0.764691172793198 - Error: 0.944236059014754 ## Pesos: 445.150000000067 ## Pesos: -395.447218931033 ## Pesos: 51.4265507429982 ## Pesos: -57.9234899928842 ## Tasa de aciertos en test: 0.791208791208791 ## Partición: 9 ## Epoca: 1 - Tasa: 0.7808047988003 - Error: 0.937765558610347 ## Pesos: 45.6999999999995 ## Pesos: -40.3354479310315 ## Pesos: 4.53343024299817 ## Pesos: -7.30984449288419 ## Tasa de aciertos en test: 0.75975975975976 ## Partición: 10 ## Epoca: 1 - Tasa: 0.77344336084021 - Error: 0.966241560390098 ## Pesos: 47.2999999999995 ## Pesos: -41.9499324310315 ## Pesos: 4.83560474299818 ## Pesos: -7.05188599288419 ## Tasa de aciertos en test: 0.775224775224775 print(glue::glue(&quot;Tasa de aciertos media en test: {tasaMedia/10}&quot;)) ## Tasa de aciertos media en test: 0.772897518097518 1.2.3 Preguntas ¿Qué beneficio supone el uso de validación cruzada? El uso de validación cruzada nos permite usar todos los datos para test y poder evaluar de mejor manera si el método utilizado para la generación del modelo es bueno. ¿Qué ocurre con la tasa de acierto del perceptrón para los diferentes datasets del ejercicio 2b? Analice el desempeño al incrementarse la dispersión de los datos. Al aumentar la dispersión de los datos, disminuye la tasa de aciertos porque el plano no permite separar correctamente las clases. 1.3 Ejercicio 3 El algoritmo implementado se encuentra en el archivo “PerceptronMulticapa.R” Lectura de datos concentlite &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/concentlite.csv&quot;, col_names = FALSE) Gráfica de datos con clase ggplot(data=concentlite, aes(x=X1, y=X2,color=X3))+geom_point() Entrenamiento de perceptrón if (calcular) { resultado &lt;- entrenarPerceptronM(concentlite, maxEpocas = 500, critFinalizacion = 0.9, nu=0.1, arquitectura = c(3,1)) } concentlite &lt;- cbind(concentlite,resultado$resultado) Gráfica con clasificación del perceptron ggplot(data=concentlite, aes(x=X1, y=X2, color=X3, shape=as.factor(resultado$resultado)))+geom_point() plot(resultado$error) # Tasa de aciertos resultado$tasa ## [1] 0.9327731 # Cantidad de epocas resultado$epocas ## [1] 296 1.3.1 Punto b Incorporación del termino de momento. Se agrega una variable alfa a la función, si la variable es cero no aplica el termino de momento. if (calcular) { resultado2 &lt;- entrenarPerceptronM(concentlite[,1:3], maxEpocas = 500, critFinalizacion = 0.9, nu=0.1, arquitectura = c(3,1), alfa = 0.5) } plot(resultado2$error) # Tasa de aciertos resultado2$tasa ## [1] 0.9147659 # Cantidad de epocas resultado2$epocas ## [1] 275 1.3.2 Punto c # Calculo punto medio concentliteMedio &lt;- array() concentliteMedio[1] &lt;- mean(concentlite$X1) concentliteMedio[2] &lt;- mean(concentlite$X2) # Agrego una columna con la distancia concentlite$dist &lt;- sqrt((concentlite$X1 - concentliteMedio[1])^2 + (concentlite$X2 - concentliteMedio[2])^2) datos &lt;- as.data.frame(cbind(concentlite[,&quot;dist&quot;], concentlite[,&quot;X3&quot;])) # Entreno perceptrón simple if (calcular) { pesosConcentlite &lt;- entrenarPerceptronSigmo(datos, maxEpocas = 100, critFinalizacion = 0.9, nu = 0.01) } # Modelo obtenido pesosConcentlite$W ## [,1] ## [1,] 1.707805 ## [2,] 6.819943 # Tasa pesosConcentlite$tasa ## [1] 0.9015606 # Gráfica de error plot(pesosConcentlite$error) # Guardamos los modelos generados if (calcular) { save(resultado,resultado2,salidaPerceptronOR,salidaPerceptronXOR, pesosConcentlite,file = &quot;resultados.RData&quot;) } 1.3.3 Preguntas: ¿Cuál es la arquitectura mínima que emplearía para resolver este problema? ¿Por qué? La arquitectura mínima es una capa de 3 neuronas y una capa de 1 neurona, esta fue la arquitectura utilizado para resolver la guía. Al estar una de las clases rodeada o contenida dentro de la otra clase, se requiere una arquitectura que genere una zona cerrada, para esto se requieren al menos tres rectas que definen una zona triangular. La arquitectura utilizada genera tres rectas con la primera capa de 3 neuronas y luego una capa de 1 neurona que a partir de la salida de las anteriores clasifica según si está dentro o fuera de esta zona triangular. ¿El número de épocas requerido para entrenar un perceptrón multicapa se modifica al emplear el término de momento? Analice el efecto de este término de momento sobre el entrenamiento. El número de época al utilizar termino de momento se reduce. En nuestro caso, se ve que disminuye de 296 a 275. Para lograr esto se agrega un término a la corrección de los pesos utilizando el error del patrón anterior. ¿Qué conclusión puede sacar a partir del ejercicio 3c? Del ejercicio 3c podemos concluir que es muy importante conocer todas las herramientas y entender bien el problema, en este caso el problema se podía resolver con una tasa de acierto mayor al 90% utilizando un perceptrón simple. Esto reduce los tiempos de entrenamiento y la complejidad del modelo. 1.4 Ejercicio 4 En ejercicio 4 quedó un head(30) para disminuir la cantidad de combinaciones de leave2out Lectura de los patrones de entrenamiento irisbin &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/irisbin.csv&quot;, col_names = FALSE) %&gt;% head(30) Selección de parámetros y entrenamiento de perceptrón Utilizamos las funciones implementadas en el archivo “PerceptronMulticapa.R” 1.4.1 Leave One Out if(calcular) { system.time({ errores_leave_one_out &lt;- foreach(e = leavePOutSplit(irisbin, 1), .combine = c) %dopar% { #para ejecucion secuencia %do% { y_real &lt;- e$test[5:7] %&gt;% as.matrix() modelo &lt;- entrenarPerceptronM(e$train, maxEpocas = 100, critFinalizacion = 0.8, arquitectura = c(4,3,3)) y_predicha &lt;- modelo$predecir(e$test) e &lt;- y_real - y_predicha error &lt;- e %*% t(e) error } }) } La media de los errores leave_one_out es: 1.0020814 El desvio standard de los errores leave_one_out es: 1.1134564 errores_leave_one_out %&gt;% data.frame(x=.) %&gt;% ggplot(aes(x=x)) + geom_density() + ggtitle(&quot;Densidad del error&quot;, subtitle = &quot;leave 1 out&quot;) Figure 1.1: grafica del error 1.4.2 Leave 2 Out if(calcular) { system.time({ split &lt;- leavePOutSplit(irisbin, 2) errores_leave_2_out &lt;- foreach(e = split, .combine = c) %dopar% { #para ejecucion secuencia %do% { y_real &lt;- e$test[5:7] %&gt;% as.matrix() modelo &lt;- entrenarPerceptronM(e$train, maxEpocas = 100, critFinalizacion = 0.7, arquitectura = c(4,3,3)) print(glue::glue(&quot;progreso: {e$actual}/{e$nroCombinaciones}&quot;)) y_predicha &lt;- modelo$predecir(e$test) e &lt;- y_real - y_predicha error &lt;- c(e[1,] %*% e[1,], e[2,] %*% e[2,]) error } }) } La media de los errores leave_one_out es: 1.0020814 El desvio standard de los errores leave_one_out es: 1.1134564 errores_leave_2_out %&gt;% data.frame(x=.) %&gt;% ggplot(aes(x=x)) + geom_density() + ggtitle(&quot;Densidad del error&quot;, subtitle = &quot;leave 2 out&quot;) # Guardamos los resultados calculados if (calcular) { save(errores_leave_one_out, errores_leave_2_out, file = &quot;resultados_ejercicio4.RData&quot;) } 1.4.3 Respuestas ¿Cuál es la arquitectura mı́nima que emplearı́a para resolver este problema? Justifique. La arquitectura mínima requerida para resolver este problema es una de 3 capas, sindo las capas de entrada y salida determinadas en su número por la formulación del problema (4 neuronas en la capa de entrada y 3 en la de salida). Una arquitectura de 3 capas permite separar regiones de decisión de arbitraria complejidad, requirida por la distribución de los datos como se aprecia en la siguiente figura: irisbin %&gt;% mutate(clase = case_when( X5 == -1 &amp; X6 == -1 &amp; X7 == 1 ~ &quot;setosa&quot;, X5 == -1 &amp; X6 == 1 &amp; X7 == -1 ~ &quot;versicolor&quot;, X5 == 1 &amp; X6 == -1 &amp; X7 == -1 ~ &quot;virginica&quot;, )) %&gt;% ggplot(aes(x=X1, y=X2, color=clase)) + geom_point() ¿Qué ventajas y desventajas poseen los métodos leave-k-out y leave-one-out para validación? La principal ventaja es que son métodos exhaustivos, por lo que usan todos los datos disponibles, resultando de gran utilidad cuando los mismos son escasos. La principal desventaja es el costo computacional de entrenar tantas veces el modelo, para \\(n\\) el número de datos y \\(k\\) el número de observaciones que se usan en cada test, el número de ciclos de entrenamiento/test es la combinatoria \\((_p^n)\\). "],
["guia-2.html", "Guía 2 Guía 2 2.1 Ejercicio 1 2.2 Ejercicio 2 2.3 Ejercicio 3 2.4 Ejercicio 4", " Guía 2 Guía 2 2.1 Ejercicio 1 2.1.1 Resolución del problema XOR con una red neuronal RBF Lectura de los patrones de entrenamiento XOR_trn &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/XOR_trn.csv&quot;, col_names = FALSE) XOR_tst &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/XOR_tst.csv&quot;, col_names = FALSE) Selección de parámetros y entrenamiento de perceptrón En este caso se utilizan 4 gausianas por la distribución de los datos. datos_x &lt;- XOR_trn[,c(1,2)] datos_y &lt;- XOR_trn[,3] modeloRBF &lt;- redRBF(datos_x, datos_y, nroGausianas = 4, funcion = &quot;sigmo&quot;) ## Epoca: 1 - Tasa: 0.749 - Error: 0.975136931564991 ## Epoca: 2 - Tasa: 1 - Error: 0.818319329810593 Prueba con datos de test datos_x &lt;- XOR_tst[,c(1,2)] datos_y &lt;- XOR_tst[,3] salida &lt;- aplicarRedRBF(modeloRBF, datos_x, datos_y) salida$tasa ## [1] 1 2.1.2 Resolución del problema Iris con una red neuronal RBF Lectura de los patrones de entrenamiento irisbin &lt;- read_csv(&quot;../../PUBLICO/Encuentro 1/Práctica/data/irisbin.csv&quot;, col_names = FALSE) Selección de parámetros y entrenamiento de perceptrón datos_x &lt;- irisbin[,c(1,2,3,4)] datos_y &lt;- irisbin[,c(5,6,7)] #Se podría aplicar criterio de Elbow para la selección de K modeloRBF &lt;- redRBF(datos_x, datos_y, nroGausianas = 10, funcion = &quot;sigmo&quot;, pnu = 0.2, pepoca = 200, pcritFinalizacion = 0.9) ## Epoca: 1 - Tasa: 0.7 - Error: 0.89449489107328 ## Epoca: 2 - Tasa: 0.7 - Error: 0.864093486816453 ## Epoca: 3 - Tasa: 0.7 - Error: 0.862629752685685 ## Epoca: 4 - Tasa: 0.7 - Error: 0.861197468977281 ## Epoca: 5 - Tasa: 0.7 - Error: 0.859768029477902 ## Epoca: 6 - Tasa: 0.7 - Error: 0.858341301170202 ## Epoca: 7 - Tasa: 0.7 - Error: 0.856917318865837 ## Epoca: 8 - Tasa: 0.7 - Error: 0.855496117547486 ## Epoca: 9 - Tasa: 0.7 - Error: 0.854077731382982 ## Epoca: 10 - Tasa: 0.7 - Error: 0.852662193726943 ## Epoca: 11 - Tasa: 0.7 - Error: 0.851249537128268 ## Epoca: 12 - Tasa: 0.7 - Error: 0.849839793337777 ## Epoca: 13 - Tasa: 0.7 - Error: 0.848432993315936 ## Epoca: 14 - Tasa: 0.7 - Error: 0.847029167240685 ## Epoca: 15 - Tasa: 0.7 - Error: 0.845628344515339 ## Epoca: 16 - Tasa: 0.7 - Error: 0.844230553776579 ## Epoca: 17 - Tasa: 0.7 - Error: 0.842835822902504 ## Epoca: 18 - Tasa: 0.7 - Error: 0.841444179020752 ## Epoca: 19 - Tasa: 0.7 - Error: 0.840055648516686 ## Epoca: 20 - Tasa: 0.7 - Error: 0.83867025704163 ## Epoca: 21 - Tasa: 0.7 - Error: 0.837288029521153 ## Epoca: 22 - Tasa: 0.7 - Error: 0.8359089901634 ## Epoca: 23 - Tasa: 0.7 - Error: 0.834533162467464 ## Epoca: 24 - Tasa: 0.7 - Error: 0.833160569231785 ## Epoca: 25 - Tasa: 0.7 - Error: 0.831791232562583 ## Epoca: 26 - Tasa: 0.7 - Error: 0.830425173882317 ## Epoca: 27 - Tasa: 0.7 - Error: 0.829062413938159 ## Epoca: 28 - Tasa: 0.7 - Error: 0.827702972810487 ## Epoca: 29 - Tasa: 0.7 - Error: 0.826346869921388 ## Epoca: 30 - Tasa: 0.7 - Error: 0.82499412404317 ## Epoca: 31 - Tasa: 0.7 - Error: 0.823644753306874 ## Epoca: 32 - Tasa: 0.7 - Error: 0.822298775210791 ## Epoca: 33 - Tasa: 0.7 - Error: 0.820956206628968 ## Epoca: 34 - Tasa: 0.7 - Error: 0.819617063819711 ## Epoca: 35 - Tasa: 0.7 - Error: 0.818281362434076 ## Epoca: 36 - Tasa: 0.7 - Error: 0.816949117524343 ## Epoca: 37 - Tasa: 0.7 - Error: 0.815620343552478 ## Epoca: 38 - Tasa: 0.7 - Error: 0.814295054398566 ## Epoca: 39 - Tasa: 0.7 - Error: 0.812973263369226 ## Epoca: 40 - Tasa: 0.7 - Error: 0.811654983206 ## Epoca: 41 - Tasa: 0.7 - Error: 0.81034022609371 ## Epoca: 42 - Tasa: 0.7 - Error: 0.809029003668779 ## Epoca: 43 - Tasa: 0.7 - Error: 0.80772132702753 ## Epoca: 44 - Tasa: 0.7 - Error: 0.806417206734431 ## Epoca: 45 - Tasa: 0.7 - Error: 0.805116652830319 ## Epoca: 46 - Tasa: 0.7 - Error: 0.803819674840565 ## Epoca: 47 - Tasa: 0.7 - Error: 0.802526281783209 ## Epoca: 48 - Tasa: 0.7 - Error: 0.801236482177041 ## Epoca: 49 - Tasa: 0.7 - Error: 0.799950284049637 ## Epoca: 50 - Tasa: 0.7 - Error: 0.798667694945347 ## Epoca: 51 - Tasa: 0.7 - Error: 0.797388721933231 ## Epoca: 52 - Tasa: 0.7 - Error: 0.796113371614938 ## Epoca: 53 - Tasa: 0.7 - Error: 0.794841650132535 ## Epoca: 54 - Tasa: 0.7 - Error: 0.793573563176281 ## Epoca: 55 - Tasa: 0.7 - Error: 0.792309115992338 ## Epoca: 56 - Tasa: 0.7 - Error: 0.791048313390427 ## Epoca: 57 - Tasa: 0.7 - Error: 0.789791159751421 ## Epoca: 58 - Tasa: 0.7 - Error: 0.788537659034883 ## Epoca: 59 - Tasa: 0.7 - Error: 0.787287814786532 ## Epoca: 60 - Tasa: 0.7 - Error: 0.786041630145652 ## Epoca: 61 - Tasa: 0.7 - Error: 0.784799107852434 ## Epoca: 62 - Tasa: 0.7 - Error: 0.783560250255252 ## Epoca: 63 - Tasa: 0.7 - Error: 0.782325059317876 ## Epoca: 64 - Tasa: 0.7 - Error: 0.781093536626609 ## Epoca: 65 - Tasa: 0.7 - Error: 0.779865683397368 ## Epoca: 66 - Tasa: 0.7 - Error: 0.778641500482686 ## Epoca: 67 - Tasa: 0.7 - Error: 0.777420988378648 ## Epoca: 68 - Tasa: 0.7 - Error: 0.77620414723176 ## Epoca: 69 - Tasa: 0.7 - Error: 0.77499097684574 ## Epoca: 70 - Tasa: 0.7 - Error: 0.773781476688248 ## Epoca: 71 - Tasa: 0.7 - Error: 0.772575645897534 ## Epoca: 72 - Tasa: 0.7 - Error: 0.77137348328902 ## Epoca: 73 - Tasa: 0.7 - Error: 0.77017498736181 ## Epoca: 74 - Tasa: 0.7 - Error: 0.768980156305124 ## Epoca: 75 - Tasa: 0.7 - Error: 0.767788988004662 ## Epoca: 76 - Tasa: 0.7 - Error: 0.766601480048893 ## Epoca: 77 - Tasa: 0.7 - Error: 0.765417629735272 ## Epoca: 78 - Tasa: 0.7 - Error: 0.764237434076384 ## Epoca: 79 - Tasa: 0.7 - Error: 0.76306088980601 ## Epoca: 80 - Tasa: 0.7 - Error: 0.761887993385127 ## Epoca: 81 - Tasa: 0.7 - Error: 0.760718741007827 ## Epoca: 82 - Tasa: 0.7 - Error: 0.759553128607168 ## Epoca: 83 - Tasa: 0.7 - Error: 0.758391151860947 ## Epoca: 84 - Tasa: 0.7 - Error: 0.757232806197404 ## Epoca: 85 - Tasa: 0.7 - Error: 0.756078086800846 ## Epoca: 86 - Tasa: 0.7 - Error: 0.754926988617207 ## Epoca: 87 - Tasa: 0.7 - Error: 0.753779506359525 ## Epoca: 88 - Tasa: 0.7 - Error: 0.752635634513351 ## Epoca: 89 - Tasa: 0.7 - Error: 0.75149536734209 ## Epoca: 90 - Tasa: 0.7 - Error: 0.750358698892255 ## Epoca: 91 - Tasa: 0.7 - Error: 0.74922562299867 ## Epoca: 92 - Tasa: 0.7 - Error: 0.748096133289576 ## Epoca: 93 - Tasa: 0.7 - Error: 0.74697022319169 ## Epoca: 94 - Tasa: 0.7 - Error: 0.745847885935173 ## Epoca: 95 - Tasa: 0.7 - Error: 0.744729114558539 ## Epoca: 96 - Tasa: 0.7 - Error: 0.743613901913489 ## Epoca: 97 - Tasa: 0.7 - Error: 0.742502240669673 ## Epoca: 98 - Tasa: 0.7 - Error: 0.741394123319387 ## Epoca: 99 - Tasa: 0.7 - Error: 0.740289542182195 ## Epoca: 100 - Tasa: 0.7 - Error: 0.739188489409489 ## Epoca: 101 - Tasa: 0.7 - Error: 0.738090956988969 ## Epoca: 102 - Tasa: 0.7 - Error: 0.736996936749071 ## Epoca: 103 - Tasa: 0.7 - Error: 0.735906420363312 ## Epoca: 104 - Tasa: 0.7 - Error: 0.734819399354575 ## Epoca: 105 - Tasa: 0.7 - Error: 0.733735865099329 ## Epoca: 106 - Tasa: 0.7 - Error: 0.73265580883178 ## Epoca: 107 - Tasa: 0.7 - Error: 0.731579221647956 ## Epoca: 108 - Tasa: 0.7 - Error: 0.730506094509727 ## Epoca: 109 - Tasa: 0.7 - Error: 0.729436418248763 ## Epoca: 110 - Tasa: 0.7 - Error: 0.728370183570426 ## Epoca: 111 - Tasa: 0.7 - Error: 0.727307381057596 ## Epoca: 112 - Tasa: 0.7 - Error: 0.726248001174443 ## Epoca: 113 - Tasa: 0.7 - Error: 0.725192034270125 ## Epoca: 114 - Tasa: 0.7 - Error: 0.724139470582431 ## Epoca: 115 - Tasa: 0.7 - Error: 0.723090300241364 ## Epoca: 116 - Tasa: 0.7 - Error: 0.72204451327266 ## Epoca: 117 - Tasa: 0.7 - Error: 0.721002099601249 ## Epoca: 118 - Tasa: 0.7 - Error: 0.719963049054653 ## Epoca: 119 - Tasa: 0.7 - Error: 0.718927351366334 ## Epoca: 120 - Tasa: 0.7 - Error: 0.717894996178973 ## Epoca: 121 - Tasa: 0.7 - Error: 0.716865973047697 ## Epoca: 122 - Tasa: 0.7 - Error: 0.715840271443255 ## Epoca: 123 - Tasa: 0.7 - Error: 0.714817880755122 ## Epoca: 124 - Tasa: 0.7 - Error: 0.713798790294565 ## Epoca: 125 - Tasa: 0.7 - Error: 0.71278298929764 ## Epoca: 126 - Tasa: 0.7 - Error: 0.711770466928146 ## Epoca: 127 - Tasa: 0.7 - Error: 0.710761212280515 ## Epoca: 128 - Tasa: 0.7 - Error: 0.709755214382654 ## Epoca: 129 - Tasa: 0.7 - Error: 0.708752462198739 ## Epoca: 130 - Tasa: 0.7 - Error: 0.707752944631944 ## Epoca: 131 - Tasa: 0.7 - Error: 0.706756650527135 ## Epoca: 132 - Tasa: 0.7 - Error: 0.705763568673499 ## Epoca: 133 - Tasa: 0.7 - Error: 0.704773687807134 ## Epoca: 134 - Tasa: 0.7 - Error: 0.70378699661358 ## Epoca: 135 - Tasa: 0.7 - Error: 0.702803483730312 ## Epoca: 136 - Tasa: 0.7 - Error: 0.701823137749171 ## Epoca: 137 - Tasa: 0.7 - Error: 0.700845947218764 ## Epoca: 138 - Tasa: 0.7 - Error: 0.699871900646804 ## Epoca: 139 - Tasa: 0.7 - Error: 0.698900986502407 ## Epoca: 140 - Tasa: 0.7 - Error: 0.697933193218347 ## Epoca: 141 - Tasa: 0.7 - Error: 0.696968509193264 ## Epoca: 142 - Tasa: 0.7 - Error: 0.696006922793825 ## Epoca: 143 - Tasa: 0.7 - Error: 0.695048422356844 ## Epoca: 144 - Tasa: 0.7 - Error: 0.69409299619136 ## Epoca: 145 - Tasa: 0.7 - Error: 0.693140632580666 ## Epoca: 146 - Tasa: 0.7 - Error: 0.692191319784305 ## Epoca: 147 - Tasa: 0.7 - Error: 0.691245046040016 ## Epoca: 148 - Tasa: 0.7 - Error: 0.690301799565647 ## Epoca: 149 - Tasa: 0.7 - Error: 0.689361568561019 ## Epoca: 150 - Tasa: 0.7 - Error: 0.688424341209761 ## Epoca: 151 - Tasa: 0.7 - Error: 0.687490105681094 ## Epoca: 152 - Tasa: 0.7 - Error: 0.686558850131587 ## Epoca: 153 - Tasa: 0.7 - Error: 0.685630562706866 ## Epoca: 154 - Tasa: 0.7 - Error: 0.684705231543295 ## Epoca: 155 - Tasa: 0.7 - Error: 0.683782844769613 ## Epoca: 156 - Tasa: 0.7 - Error: 0.682863390508536 ## Epoca: 157 - Tasa: 0.7 - Error: 0.681946856878325 ## Epoca: 158 - Tasa: 0.7 - Error: 0.681033231994319 ## Epoca: 159 - Tasa: 0.7 - Error: 0.68012250397043 ## Epoca: 160 - Tasa: 0.7 - Error: 0.67921466092061 ## Epoca: 161 - Tasa: 0.7 - Error: 0.678309690960278 ## Epoca: 162 - Tasa: 0.7 - Error: 0.677407582207717 ## Epoca: 163 - Tasa: 0.7 - Error: 0.676508322785439 ## Epoca: 164 - Tasa: 0.7 - Error: 0.675611900821515 ## Epoca: 165 - Tasa: 0.7 - Error: 0.674718304450881 ## Epoca: 166 - Tasa: 0.7 - Error: 0.6738275218166 ## Epoca: 167 - Tasa: 0.7 - Error: 0.67293954107111 ## Epoca: 168 - Tasa: 0.7 - Error: 0.672054350377424 ## Epoca: 169 - Tasa: 0.7 - Error: 0.671171937910317 ## Epoca: 170 - Tasa: 0.7 - Error: 0.670292291857475 ## Epoca: 171 - Tasa: 0.7 - Error: 0.669415400420615 ## Epoca: 172 - Tasa: 0.7 - Error: 0.668541251816581 ## Epoca: 173 - Tasa: 0.7 - Error: 0.66766983427841 ## Epoca: 174 - Tasa: 0.7 - Error: 0.666801136056373 ## Epoca: 175 - Tasa: 0.7 - Error: 0.665935145418987 ## Epoca: 176 - Tasa: 0.7 - Error: 0.665071850653999 ## Epoca: 177 - Tasa: 0.7 - Error: 0.664211240069353 ## Epoca: 178 - Tasa: 0.7 - Error: 0.663353301994119 ## Epoca: 179 - Tasa: 0.7 - Error: 0.66249802477941 ## Epoca: 180 - Tasa: 0.7 - Error: 0.661645396799261 ## Epoca: 181 - Tasa: 0.7 - Error: 0.660795406451502 ## Epoca: 182 - Tasa: 0.7 - Error: 0.659948042158584 ## Epoca: 183 - Tasa: 0.7 - Error: 0.659103292368405 ## Epoca: 184 - Tasa: 0.7 - Error: 0.6582611455551 ## Epoca: 185 - Tasa: 0.7 - Error: 0.657421590219808 ## Epoca: 186 - Tasa: 0.7 - Error: 0.656584614891424 ## Epoca: 187 - Tasa: 0.7 - Error: 0.655750208127328 ## Epoca: 188 - Tasa: 0.7 - Error: 0.654918358514087 ## Epoca: 189 - Tasa: 0.72 - Error: 0.65408905466814 ## Epoca: 190 - Tasa: 0.72 - Error: 0.653262285236468 ## Epoca: 191 - Tasa: 0.72 - Error: 0.652438038897235 ## Epoca: 192 - Tasa: 0.72 - Error: 0.651616304360411 ## Epoca: 193 - Tasa: 0.72 - Error: 0.650797070368383 ## Epoca: 194 - Tasa: 0.72 - Error: 0.649980325696542 ## Epoca: 195 - Tasa: 0.72 - Error: 0.649166059153845 ## Epoca: 196 - Tasa: 0.72 - Error: 0.648354259583373 ## Epoca: 197 - Tasa: 0.72 - Error: 0.64754491586286 ## Epoca: 198 - Tasa: 0.72 - Error: 0.646738016905207 ## Epoca: 199 - Tasa: 0.72 - Error: 0.645933551658983 ## Epoca: 200 - Tasa: 0.72 - Error: 0.645131509108903 ## Epoca: 1 - Tasa: 0.713333333333333 - Error: 0.862884633639659 ## Epoca: 2 - Tasa: 0.713333333333333 - Error: 0.826876287596203 ## Epoca: 3 - Tasa: 0.713333333333333 - Error: 0.825286206817963 ## Epoca: 4 - Tasa: 0.713333333333333 - Error: 0.82364745916709 ## Epoca: 5 - Tasa: 0.713333333333333 - Error: 0.822011185798905 ## Epoca: 6 - Tasa: 0.713333333333333 - Error: 0.82037791805056 ## Epoca: 7 - Tasa: 0.713333333333333 - Error: 0.818747723655696 ## Epoca: 8 - Tasa: 0.713333333333333 - Error: 0.817120664853259 ## Epoca: 9 - Tasa: 0.713333333333333 - Error: 0.81549680242582 ## Epoca: 10 - Tasa: 0.713333333333333 - Error: 0.813876195747633 ## Epoca: 11 - Tasa: 0.713333333333333 - Error: 0.812258902798518 ## Epoca: 12 - Tasa: 0.713333333333333 - Error: 0.810644980177685 ## Epoca: 13 - Tasa: 0.713333333333333 - Error: 0.809034483117791 ## Epoca: 14 - Tasa: 0.713333333333333 - Error: 0.807427465499211 ## Epoca: 15 - Tasa: 0.713333333333333 - Error: 0.805823979864483 ## Epoca: 16 - Tasa: 0.713333333333333 - Error: 0.804224077432943 ## Epoca: 17 - Tasa: 0.713333333333333 - Error: 0.802627808115498 ## Epoca: 18 - Tasa: 0.713333333333333 - Error: 0.801035220529555 ## Epoca: 19 - Tasa: 0.713333333333333 - Error: 0.799446362014055 ## Epoca: 20 - Tasa: 0.713333333333333 - Error: 0.797861278644631 ## Epoca: 21 - Tasa: 0.713333333333333 - Error: 0.796280015248852 ## Epoca: 22 - Tasa: 0.713333333333333 - Error: 0.794702615421545 ## Epoca: 23 - Tasa: 0.713333333333333 - Error: 0.793129121540192 ## Epoca: 24 - Tasa: 0.713333333333333 - Error: 0.791559574780375 ## Epoca: 25 - Tasa: 0.713333333333333 - Error: 0.789994015131267 ## Epoca: 26 - Tasa: 0.713333333333333 - Error: 0.788432481411154 ## Epoca: 27 - Tasa: 0.713333333333333 - Error: 0.786875011282978 ## Epoca: 28 - Tasa: 0.713333333333333 - Error: 0.785321641269891 ## Epoca: 29 - Tasa: 0.713333333333333 - Error: 0.783772406770809 ## Epoca: 30 - Tasa: 0.713333333333333 - Error: 0.782227342075958 ## Epoca: 31 - Tasa: 0.713333333333333 - Error: 0.780686480382403 ## Epoca: 32 - Tasa: 0.713333333333333 - Error: 0.779149853809553 ## Epoca: 33 - Tasa: 0.713333333333333 - Error: 0.77761749341463 ## Epoca: 34 - Tasa: 0.713333333333333 - Error: 0.776089429208099 ## Epoca: 35 - Tasa: 0.713333333333333 - Error: 0.774565690169052 ## Epoca: 36 - Tasa: 0.713333333333333 - Error: 0.77304630426053 ## Epoca: 37 - Tasa: 0.713333333333333 - Error: 0.771531298444796 ## Epoca: 38 - Tasa: 0.713333333333333 - Error: 0.770020698698531 ## Epoca: 39 - Tasa: 0.713333333333333 - Error: 0.768514530027961 ## Epoca: 40 - Tasa: 0.713333333333333 - Error: 0.76701281648391 ## Epoca: 41 - Tasa: 0.713333333333333 - Error: 0.765515581176761 ## Epoca: 42 - Tasa: 0.713333333333333 - Error: 0.764022846291344 ## Epoca: 43 - Tasa: 0.713333333333333 - Error: 0.762534633101715 ## Epoca: 44 - Tasa: 0.713333333333333 - Error: 0.761050961985857 ## Epoca: 45 - Tasa: 0.713333333333333 - Error: 0.759571852440262 ## Epoca: 46 - Tasa: 0.713333333333333 - Error: 0.758097323094431 ## Epoca: 47 - Tasa: 0.713333333333333 - Error: 0.756627391725248 ## Epoca: 48 - Tasa: 0.713333333333333 - Error: 0.755162075271256 ## Epoca: 49 - Tasa: 0.713333333333333 - Error: 0.753701389846816 ## Epoca: 50 - Tasa: 0.713333333333333 - Error: 0.752245350756153 ## Epoca: 51 - Tasa: 0.713333333333333 - Error: 0.750793972507279 ## Epoca: 52 - Tasa: 0.713333333333333 - Error: 0.7493472688258 ## Epoca: 53 - Tasa: 0.713333333333333 - Error: 0.747905252668605 ## Epoca: 54 - Tasa: 0.713333333333333 - Error: 0.746467936237414 ## Epoca: 55 - Tasa: 0.713333333333333 - Error: 0.745035330992222 ## Epoca: 56 - Tasa: 0.713333333333333 - Error: 0.743607447664598 ## Epoca: 57 - Tasa: 0.713333333333333 - Error: 0.742184296270863 ## Epoca: 58 - Tasa: 0.713333333333333 - Error: 0.740765886125134 ## Epoca: 59 - Tasa: 0.713333333333333 - Error: 0.739352225852237 ## Epoca: 60 - Tasa: 0.713333333333333 - Error: 0.737943323400488 ## Epoca: 61 - Tasa: 0.713333333333333 - Error: 0.736539186054339 ## Epoca: 62 - Tasa: 0.713333333333333 - Error: 0.735139820446889 ## Epoca: 63 - Tasa: 0.713333333333333 - Error: 0.73374523257226 ## Epoca: 64 - Tasa: 0.713333333333333 - Error: 0.732355427797834 ## Epoca: 65 - Tasa: 0.713333333333333 - Error: 0.730970410876363 ## Epoca: 66 - Tasa: 0.713333333333333 - Error: 0.729590185957925 ## Epoca: 67 - Tasa: 0.713333333333333 - Error: 0.728214756601759 ## Epoca: 68 - Tasa: 0.713333333333333 - Error: 0.726844125787952 ## Epoca: 69 - Tasa: 0.713333333333333 - Error: 0.725478295928996 ## Epoca: 70 - Tasa: 0.713333333333333 - Error: 0.724117268881193 ## Epoca: 71 - Tasa: 0.713333333333333 - Error: 0.722761045955943 ## Epoca: 72 - Tasa: 0.713333333333333 - Error: 0.721409627930872 ## Epoca: 73 - Tasa: 0.713333333333333 - Error: 0.720063015060838 ## Epoca: 74 - Tasa: 0.713333333333333 - Error: 0.718721207088793 ## Epoca: 75 - Tasa: 0.713333333333333 - Error: 0.717384203256505 ## Epoca: 76 - Tasa: 0.713333333333333 - Error: 0.71605200231515 ## Epoca: 77 - Tasa: 0.713333333333333 - Error: 0.714724602535759 ## Epoca: 78 - Tasa: 0.713333333333333 - Error: 0.713402001719535 ## Epoca: 79 - Tasa: 0.713333333333333 - Error: 0.71208419720803 ## Epoca: 80 - Tasa: 0.713333333333333 - Error: 0.710771185893188 ## Epoca: 81 - Tasa: 0.713333333333333 - Error: 0.709462964227255 ## Epoca: 82 - Tasa: 0.713333333333333 - Error: 0.708159528232547 ## Epoca: 83 - Tasa: 0.713333333333333 - Error: 0.706860873511096 ## Epoca: 84 - Tasa: 0.713333333333333 - Error: 0.705566995254152 ## Epoca: 85 - Tasa: 0.713333333333333 - Error: 0.70427788825156 ## Epoca: 86 - Tasa: 0.713333333333333 - Error: 0.702993546900998 ## Epoca: 87 - Tasa: 0.713333333333333 - Error: 0.701713965217092 ## Epoca: 88 - Tasa: 0.713333333333333 - Error: 0.700439136840396 ## Epoca: 89 - Tasa: 0.713333333333333 - Error: 0.699169055046242 ## Epoca: 90 - Tasa: 0.713333333333333 - Error: 0.697903712753465 ## Epoca: 91 - Tasa: 0.713333333333333 - Error: 0.696643102532995 ## Epoca: 92 - Tasa: 0.713333333333333 - Error: 0.695387216616331 ## Epoca: 93 - Tasa: 0.713333333333333 - Error: 0.694136046903876 ## Epoca: 94 - Tasa: 0.713333333333333 - Error: 0.692889584973162 ## Epoca: 95 - Tasa: 0.713333333333333 - Error: 0.691647822086937 ## Epoca: 96 - Tasa: 0.713333333333333 - Error: 0.690410749201138 ## Epoca: 97 - Tasa: 0.713333333333333 - Error: 0.689178356972741 ## Epoca: 98 - Tasa: 0.713333333333333 - Error: 0.687950635767484 ## Epoca: 99 - Tasa: 0.713333333333333 - Error: 0.686727575667475 ## Epoca: 100 - Tasa: 0.713333333333333 - Error: 0.685509166478683 ## Epoca: 101 - Tasa: 0.713333333333333 - Error: 0.684295397738303 ## Epoca: 102 - Tasa: 0.713333333333333 - Error: 0.683086258722012 ## Epoca: 103 - Tasa: 0.713333333333333 - Error: 0.681881738451101 ## Epoca: 104 - Tasa: 0.713333333333333 - Error: 0.680681825699505 ## Epoca: 105 - Tasa: 0.713333333333333 - Error: 0.679486509000703 ## Epoca: 106 - Tasa: 0.713333333333333 - Error: 0.678295776654515 ## Epoca: 107 - Tasa: 0.713333333333333 - Error: 0.67710961673379 ## Epoca: 108 - Tasa: 0.713333333333333 - Error: 0.675928017090972 ## Epoca: 109 - Tasa: 0.713333333333333 - Error: 0.674750965364569 ## Epoca: 110 - Tasa: 0.713333333333333 - Error: 0.673578448985506 ## Epoca: 111 - Tasa: 0.713333333333333 - Error: 0.672410455183374 ## Epoca: 112 - Tasa: 0.713333333333333 - Error: 0.671246970992569 ## Epoca: 113 - Tasa: 0.713333333333333 - Error: 0.670087983258332 ## Epoca: 114 - Tasa: 0.713333333333333 - Error: 0.668933478642681 ## Epoca: 115 - Tasa: 0.713333333333333 - Error: 0.667783443630241 ## Epoca: 116 - Tasa: 0.713333333333333 - Error: 0.666637864533971 ## Epoca: 117 - Tasa: 0.713333333333333 - Error: 0.665496727500797 ## Epoca: 118 - Tasa: 0.713333333333333 - Error: 0.664360018517135 ## Epoca: 119 - Tasa: 0.713333333333333 - Error: 0.663227723414326 ## Epoca: 120 - Tasa: 0.713333333333333 - Error: 0.662099827873966 ## Epoca: 121 - Tasa: 0.713333333333333 - Error: 0.660976317433146 ## Epoca: 122 - Tasa: 0.713333333333333 - Error: 0.659857177489595 ## Epoca: 123 - Tasa: 0.713333333333333 - Error: 0.658742393306726 ## Epoca: 124 - Tasa: 0.713333333333333 - Error: 0.657631950018597 ## Epoca: 125 - Tasa: 0.713333333333333 - Error: 0.656525832634772 ## Epoca: 126 - Tasa: 0.713333333333333 - Error: 0.655424026045097 ## Epoca: 127 - Tasa: 0.713333333333333 - Error: 0.654326515024385 ## Epoca: 128 - Tasa: 0.713333333333333 - Error: 0.653233284237016 ## Epoca: 129 - Tasa: 0.713333333333333 - Error: 0.652144318241445 ## Epoca: 130 - Tasa: 0.713333333333333 - Error: 0.651059601494624 ## Epoca: 131 - Tasa: 0.713333333333333 - Error: 0.649979118356349 ## Epoca: 132 - Tasa: 0.713333333333333 - Error: 0.648902853093509 ## Epoca: 133 - Tasa: 0.713333333333333 - Error: 0.647830789884263 ## Epoca: 134 - Tasa: 0.713333333333333 - Error: 0.64676291282213 ## Epoca: 135 - Tasa: 0.713333333333333 - Error: 0.645699205920004 ## Epoca: 136 - Tasa: 0.713333333333333 - Error: 0.644639653114083 ## Epoca: 137 - Tasa: 0.713333333333333 - Error: 0.643584238267723 ## Epoca: 138 - Tasa: 0.713333333333333 - Error: 0.642532945175219 ## Epoca: 139 - Tasa: 0.713333333333333 - Error: 0.641485757565499 ## Epoca: 140 - Tasa: 0.713333333333333 - Error: 0.640442659105757 ## Epoca: 141 - Tasa: 0.713333333333333 - Error: 0.639403633404999 ## Epoca: 142 - Tasa: 0.713333333333333 - Error: 0.638368664017523 ## Epoca: 143 - Tasa: 0.713333333333333 - Error: 0.637337734446328 ## Epoca: 144 - Tasa: 0.713333333333333 - Error: 0.636310828146449 ## Epoca: 145 - Tasa: 0.713333333333333 - Error: 0.635287928528219 ## Epoca: 146 - Tasa: 0.713333333333333 - Error: 0.634269018960473 ## Epoca: 147 - Tasa: 0.713333333333333 - Error: 0.63325408277367 ## Epoca: 148 - Tasa: 0.713333333333333 - Error: 0.632243103262961 ## Epoca: 149 - Tasa: 0.713333333333333 - Error: 0.631236063691182 ## Epoca: 150 - Tasa: 0.713333333333333 - Error: 0.630232947291787 ## Epoca: 151 - Tasa: 0.713333333333333 - Error: 0.629233737271714 ## Epoca: 152 - Tasa: 0.713333333333333 - Error: 0.628238416814192 ## Epoca: 153 - Tasa: 0.713333333333333 - Error: 0.627246969081482 ## Epoca: 154 - Tasa: 0.713333333333333 - Error: 0.626259377217558 ## Epoca: 155 - Tasa: 0.713333333333333 - Error: 0.625275624350731 ## Epoca: 156 - Tasa: 0.713333333333333 - Error: 0.624295693596207 ## Epoca: 157 - Tasa: 0.713333333333333 - Error: 0.623319568058595 ## Epoca: 158 - Tasa: 0.713333333333333 - Error: 0.622347230834353 ## Epoca: 159 - Tasa: 0.713333333333333 - Error: 0.621378665014176 ## Epoca: 160 - Tasa: 0.713333333333333 - Error: 0.620413853685332 ## Epoca: 161 - Tasa: 0.713333333333333 - Error: 0.619452779933943 ## Epoca: 162 - Tasa: 0.713333333333333 - Error: 0.618495426847209 ## Epoca: 163 - Tasa: 0.713333333333333 - Error: 0.617541777515583 ## Epoca: 164 - Tasa: 0.713333333333333 - Error: 0.616591815034894 ## Epoca: 165 - Tasa: 0.713333333333333 - Error: 0.615645522508413 ## Epoca: 166 - Tasa: 0.713333333333333 - Error: 0.614702883048874 ## Epoca: 167 - Tasa: 0.713333333333333 - Error: 0.613763879780449 ## Epoca: 168 - Tasa: 0.713333333333333 - Error: 0.612828495840663 ## Epoca: 169 - Tasa: 0.713333333333333 - Error: 0.611896714382272 ## Epoca: 170 - Tasa: 0.713333333333333 - Error: 0.610968518575087 ## Epoca: 171 - Tasa: 0.713333333333333 - Error: 0.61004389160776 ## Epoca: 172 - Tasa: 0.713333333333333 - Error: 0.60912281668951 ## Epoca: 173 - Tasa: 0.713333333333333 - Error: 0.608205277051821 ## Epoca: 174 - Tasa: 0.713333333333333 - Error: 0.607291255950088 ## Epoca: 175 - Tasa: 0.713333333333333 - Error: 0.606380736665215 ## Epoca: 176 - Tasa: 0.713333333333333 - Error: 0.605473702505182 ## Epoca: 177 - Tasa: 0.713333333333333 - Error: 0.604570136806558 ## Epoca: 178 - Tasa: 0.713333333333333 - Error: 0.603670022935987 ## Epoca: 179 - Tasa: 0.713333333333333 - Error: 0.602773344291621 ## Epoca: 180 - Tasa: 0.713333333333333 - Error: 0.60188008430452 ## Epoca: 181 - Tasa: 0.713333333333333 - Error: 0.60099022644001 ## Epoca: 182 - Tasa: 0.713333333333333 - Error: 0.600103754199012 ## Epoca: 183 - Tasa: 0.713333333333333 - Error: 0.599220651119317 ## Epoca: 184 - Tasa: 0.713333333333333 - Error: 0.59834090077684 ## Epoca: 185 - Tasa: 0.713333333333333 - Error: 0.597464486786833 ## Epoca: 186 - Tasa: 0.713333333333333 - Error: 0.596591392805057 ## Epoca: 187 - Tasa: 0.713333333333333 - Error: 0.595721602528926 ## Epoca: 188 - Tasa: 0.713333333333333 - Error: 0.594855099698615 ## Epoca: 189 - Tasa: 0.713333333333333 - Error: 0.593991868098136 ## Epoca: 190 - Tasa: 0.713333333333333 - Error: 0.593131891556378 ## Epoca: 191 - Tasa: 0.713333333333333 - Error: 0.592275153948114 ## Epoca: 192 - Tasa: 0.713333333333333 - Error: 0.591421639194985 ## Epoca: 193 - Tasa: 0.713333333333333 - Error: 0.590571331266443 ## Epoca: 194 - Tasa: 0.713333333333333 - Error: 0.589724214180667 ## Epoca: 195 - Tasa: 0.713333333333333 - Error: 0.588880272005453 ## Epoca: 196 - Tasa: 0.713333333333333 - Error: 0.588039488859071 ## Epoca: 197 - Tasa: 0.713333333333333 - Error: 0.587201848911092 ## Epoca: 198 - Tasa: 0.713333333333333 - Error: 0.58636733638319 ## Epoca: 199 - Tasa: 0.713333333333333 - Error: 0.58553593554992 ## Epoca: 200 - Tasa: 0.713333333333333 - Error: 0.584707630739457 ## Epoca: 1 - Tasa: 0.586666666666667 - Error: 0.996200433769754 ## Epoca: 2 - Tasa: 0.586666666666667 - Error: 0.981061928437062 ## Epoca: 3 - Tasa: 0.586666666666667 - Error: 0.974549256096245 ## Epoca: 4 - Tasa: 0.586666666666667 - Error: 0.968060481695876 ## Epoca: 5 - Tasa: 0.586666666666667 - Error: 0.961606243662866 ## Epoca: 6 - Tasa: 0.586666666666667 - Error: 0.95518709594034 ## Epoca: 7 - Tasa: 0.586666666666667 - Error: 0.948803550917149 ## Epoca: 8 - Tasa: 0.586666666666667 - Error: 0.94245609206567 ## Epoca: 9 - Tasa: 0.586666666666667 - Error: 0.93614517481774 ## Epoca: 10 - Tasa: 0.586666666666667 - Error: 0.929871227401144 ## Epoca: 11 - Tasa: 0.586666666666667 - Error: 0.923634651650923 ## Epoca: 12 - Tasa: 0.586666666666667 - Error: 0.917435823796739 ## Epoca: 13 - Tasa: 0.586666666666667 - Error: 0.911275095227457 ## Epoca: 14 - Tasa: 0.586666666666667 - Error: 0.905152793234026 ## Epoca: 15 - Tasa: 0.586666666666667 - Error: 0.899069221731699 ## Epoca: 16 - Tasa: 0.586666666666667 - Error: 0.89302466196253 ## Epoca: 17 - Tasa: 0.586666666666667 - Error: 0.887019373179061 ## Epoca: 18 - Tasa: 0.586666666666667 - Error: 0.881053593310033 ## Epoca: 19 - Tasa: 0.586666666666667 - Error: 0.87512753960889 ## Epoca: 20 - Tasa: 0.586666666666667 - Error: 0.869241409285819 ## Epoca: 21 - Tasa: 0.586666666666667 - Error: 0.863395380123982 ## Epoca: 22 - Tasa: 0.586666666666667 - Error: 0.857589611080579 ## Epoca: 23 - Tasa: 0.586666666666667 - Error: 0.851824242873309 ## Epoca: 24 - Tasa: 0.586666666666667 - Error: 0.846099398552777 ## Epoca: 25 - Tasa: 0.586666666666667 - Error: 0.84041518406133 ## Epoca: 26 - Tasa: 0.586666666666667 - Error: 0.834771688778795 ## Epoca: 27 - Tasa: 0.586666666666667 - Error: 0.829168986055531 ## Epoca: 28 - Tasa: 0.586666666666667 - Error: 0.823607133733191 ## Epoca: 29 - Tasa: 0.586666666666667 - Error: 0.818086174653564 ## Epoca: 30 - Tasa: 0.586666666666667 - Error: 0.812606137155816 ## Epoca: 31 - Tasa: 0.586666666666667 - Error: 0.807167035562458 ## Epoca: 32 - Tasa: 0.586666666666667 - Error: 0.801768870654318 ## Epoca: 33 - Tasa: 0.586666666666667 - Error: 0.796411630134785 ## Epoca: 34 - Tasa: 0.586666666666667 - Error: 0.791095289083579 ## Epoca: 35 - Tasa: 0.586666666666667 - Error: 0.785819810400267 ## Epoca: 36 - Tasa: 0.586666666666667 - Error: 0.780585145237752 ## Epoca: 37 - Tasa: 0.586666666666667 - Error: 0.775391233425932 ## Epoca: 38 - Tasa: 0.586666666666667 - Error: 0.770238003885713 ## Epoca: 39 - Tasa: 0.586666666666667 - Error: 0.765125375033561 ## Epoca: 40 - Tasa: 0.586666666666667 - Error: 0.760053255176761 ## Epoca: 41 - Tasa: 0.586666666666667 - Error: 0.755021542899537 ## Epoca: 42 - Tasa: 0.586666666666667 - Error: 0.750030127440187 ## Epoca: 43 - Tasa: 0.586666666666667 - Error: 0.745078889059381 ## Epoca: 44 - Tasa: 0.586666666666667 - Error: 0.740167699399757 ## Epoca: 45 - Tasa: 0.586666666666667 - Error: 0.735296421836948 ## Epoca: 46 - Tasa: 0.586666666666667 - Error: 0.730464911822175 ## Epoca: 47 - Tasa: 0.586666666666667 - Error: 0.725673017216525 ## Epoca: 48 - Tasa: 0.586666666666667 - Error: 0.72092057861704 ## Epoca: 49 - Tasa: 0.586666666666667 - Error: 0.716207429674736 ## Epoca: 50 - Tasa: 0.586666666666667 - Error: 0.711533397404672 ## Epoca: 51 - Tasa: 0.586666666666667 - Error: 0.706898302488175 ## Epoca: 52 - Tasa: 0.586666666666667 - Error: 0.702301959567348 ## Epoca: 53 - Tasa: 0.586666666666667 - Error: 0.697744177531969 ## Epoca: 54 - Tasa: 0.586666666666667 - Error: 0.693224759798884 ## Epoca: 55 - Tasa: 0.586666666666667 - Error: 0.688743504584026 ## Epoca: 56 - Tasa: 0.586666666666667 - Error: 0.684300205167145 ## Epoca: 57 - Tasa: 0.586666666666667 - Error: 0.679894650149384 ## Epoca: 58 - Tasa: 0.586666666666667 - Error: 0.675526623703794 ## Epoca: 59 - Tasa: 0.586666666666667 - Error: 0.671195905818909 ## Epoca: 60 - Tasa: 0.586666666666667 - Error: 0.666902272535487 ## Epoca: 61 - Tasa: 0.586666666666667 - Error: 0.662645496176522 ## Epoca: 62 - Tasa: 0.686666666666667 - Error: 0.658425345570655 ## Epoca: 63 - Tasa: 0.686666666666667 - Error: 0.654241586269072 ## Epoca: 64 - Tasa: 0.713333333333333 - Error: 0.650093980756019 ## Epoca: 65 - Tasa: 0.753333333333333 - Error: 0.645982288653036 ## Epoca: 66 - Tasa: 0.806666666666667 - Error: 0.641906266917016 ## Epoca: 67 - Tasa: 0.84 - Error: 0.63786567003222 ## Epoca: 68 - Tasa: 0.866666666666667 - Error: 0.633860250196331 ## Epoca: 69 - Tasa: 0.866666666666667 - Error: 0.629889757500689 ## Epoca: 70 - Tasa: 0.866666666666667 - Error: 0.625953940104789 ## Epoca: 71 - Tasa: 0.893333333333333 - Error: 0.62205254440518 ## Epoca: 72 - Tasa: 0.9 - Error: 0.618185315198853 ## Epoca: 73 - Tasa: 0.906666666666667 - Error: 0.614351995841245 Prueba con datos salida &lt;- aplicarRedRBF(modeloRBF, datos_x, datos_y) salida$tasa ## [1] 0.94 head(salida$salida) ## 1 1 1 ## 1 -1 -1 1 ## 2 -1 -1 1 ## 3 1 -1 -1 ## 4 -1 -1 1 ## 5 1 -1 -1 ## 6 1 -1 -1 Cantidad de parámetros: En una red MLP con una estructura (3,1), tenemos los siguientes parámetros: \\(numParamMLP = Parámetros de Capa 1 + Parámetros de Capa 2\\) \\(numParamMLP = [(4 entradas + 1) * 3 neuronas] + [(3 entradas + 1) * 1 neurona]\\) \\(numParamMLP = 5 * 3 + 4 * 1 = 19 parámetros\\) Una red RBF con 19 parámetros podría tener la siguiente distribución: \\(numParamRBF = Parámetros de Gausianas + Parámetros de Perceptrones\\) \\(numParamRBF = [3 centros] + [(3 entradas + 1) * 3 neurona]\\) \\(numParamRBF = 3 + 4 * 3 = 15 parámetros\\) modeloRBF_2 &lt;- redRBF(datos_x, datos_y, nroGausianas = 3, funcion = &quot;sigmo&quot;, pnu = 0.2, pepoca = 200, pcritFinalizacion = 0.9) ## Epoca: 1 - Tasa: 0.7 - Error: 0.898137153324026 ## Epoca: 2 - Tasa: 0.7 - Error: 0.868279769865699 ## Epoca: 3 - Tasa: 0.7 - Error: 0.86759353466148 ## Epoca: 4 - Tasa: 0.7 - Error: 0.866937620211468 ## Epoca: 5 - Tasa: 0.7 - Error: 0.866282238303869 ## Epoca: 6 - Tasa: 0.7 - Error: 0.865627207456794 ## Epoca: 7 - Tasa: 0.7 - Error: 0.86497253055321 ## Epoca: 8 - Tasa: 0.7 - Error: 0.864318211608235 ## Epoca: 9 - Tasa: 0.7 - Error: 0.863664254617286 ## Epoca: 10 - Tasa: 0.7 - Error: 0.863010663548921 ## Epoca: 11 - Tasa: 0.7 - Error: 0.862357442344828 ## Epoca: 12 - Tasa: 0.7 - Error: 0.861704594919853 ## Epoca: 13 - Tasa: 0.7 - Error: 0.861052125162038 ## Epoca: 14 - Tasa: 0.7 - Error: 0.860400036932651 ## Epoca: 15 - Tasa: 0.7 - Error: 0.859748334066231 ## Epoca: 16 - Tasa: 0.7 - Error: 0.859097020370615 ## Epoca: 17 - Tasa: 0.7 - Error: 0.858446099626986 ## Epoca: 18 - Tasa: 0.7 - Error: 0.857795575589908 ## Epoca: 19 - Tasa: 0.7 - Error: 0.857145451987368 ## Epoca: 20 - Tasa: 0.7 - Error: 0.85649573252082 ## Epoca: 21 - Tasa: 0.7 - Error: 0.855846420865225 ## Epoca: 22 - Tasa: 0.7 - Error: 0.855197520669102 ## Epoca: 23 - Tasa: 0.7 - Error: 0.854549035554567 ## Epoca: 24 - Tasa: 0.7 - Error: 0.853900969117384 ## Epoca: 25 - Tasa: 0.7 - Error: 0.85325332492701 ## Epoca: 26 - Tasa: 0.7 - Error: 0.852606106526648 ## Epoca: 27 - Tasa: 0.7 - Error: 0.851959317433293 ## Epoca: 28 - Tasa: 0.7 - Error: 0.851312961137785 ## Epoca: 29 - Tasa: 0.7 - Error: 0.850667041104863 ## Epoca: 30 - Tasa: 0.7 - Error: 0.850021560773214 ## Epoca: 31 - Tasa: 0.7 - Error: 0.849376523555528 ## Epoca: 32 - Tasa: 0.7 - Error: 0.848731932838559 ## Epoca: 33 - Tasa: 0.7 - Error: 0.84808779198317 ## Epoca: 34 - Tasa: 0.7 - Error: 0.847444104324402 ## Epoca: 35 - Tasa: 0.7 - Error: 0.846800873171522 ## Epoca: 36 - Tasa: 0.7 - Error: 0.846158101808087 ## Epoca: 37 - Tasa: 0.7 - Error: 0.845515793492005 ## Epoca: 38 - Tasa: 0.7 - Error: 0.844873951455589 ## Epoca: 39 - Tasa: 0.7 - Error: 0.844232578905626 ## Epoca: 40 - Tasa: 0.7 - Error: 0.843591679023433 ## Epoca: 41 - Tasa: 0.7 - Error: 0.842951254964927 ## Epoca: 42 - Tasa: 0.7 - Error: 0.84231130986068 ## Epoca: 43 - Tasa: 0.7 - Error: 0.841671846815993 ## Epoca: 44 - Tasa: 0.7 - Error: 0.841032868910955 ## Epoca: 45 - Tasa: 0.7 - Error: 0.840394379200511 ## Epoca: 46 - Tasa: 0.7 - Error: 0.839756380714533 ## Epoca: 47 - Tasa: 0.7 - Error: 0.839118876457881 ## Epoca: 48 - Tasa: 0.7 - Error: 0.838481869410476 ## Epoca: 49 - Tasa: 0.7 - Error: 0.837845362527371 ## Epoca: 50 - Tasa: 0.7 - Error: 0.837209358738815 ## Epoca: 51 - Tasa: 0.7 - Error: 0.836573860950329 ## Epoca: 52 - Tasa: 0.7 - Error: 0.835938872042777 ## Epoca: 53 - Tasa: 0.7 - Error: 0.835304394872434 ## Epoca: 54 - Tasa: 0.7 - Error: 0.834670432271064 ## Epoca: 55 - Tasa: 0.7 - Error: 0.834036987045989 ## Epoca: 56 - Tasa: 0.7 - Error: 0.833404061980168 ## Epoca: 57 - Tasa: 0.7 - Error: 0.832771659832265 ## Epoca: 58 - Tasa: 0.7 - Error: 0.832139783336732 ## Epoca: 59 - Tasa: 0.7 - Error: 0.831508435203878 ## Epoca: 60 - Tasa: 0.7 - Error: 0.830877618119949 ## Epoca: 61 - Tasa: 0.7 - Error: 0.830247334747207 ## Epoca: 62 - Tasa: 0.7 - Error: 0.829617587724001 ## Epoca: 63 - Tasa: 0.7 - Error: 0.828988379664853 ## Epoca: 64 - Tasa: 0.7 - Error: 0.828359713160531 ## Epoca: 65 - Tasa: 0.7 - Error: 0.827731590778131 ## Epoca: 66 - Tasa: 0.7 - Error: 0.827104015061157 ## Epoca: 67 - Tasa: 0.7 - Error: 0.826476988529598 ## Epoca: 68 - Tasa: 0.7 - Error: 0.825850513680014 ## Epoca: 69 - Tasa: 0.7 - Error: 0.825224592985612 ## Epoca: 70 - Tasa: 0.7 - Error: 0.824599228896331 ## Epoca: 71 - Tasa: 0.7 - Error: 0.823974423838925 ## Epoca: 72 - Tasa: 0.7 - Error: 0.823350180217042 ## Epoca: 73 - Tasa: 0.7 - Error: 0.822726500411308 ## Epoca: 74 - Tasa: 0.7 - Error: 0.822103386779415 ## Epoca: 75 - Tasa: 0.7 - Error: 0.8214808416562 ## Epoca: 76 - Tasa: 0.7 - Error: 0.820858867353729 ## Epoca: 77 - Tasa: 0.7 - Error: 0.820237466161387 ## Epoca: 78 - Tasa: 0.7 - Error: 0.819616640345957 ## Epoca: 79 - Tasa: 0.7 - Error: 0.81899639215171 ## Epoca: 80 - Tasa: 0.7 - Error: 0.818376723800489 ## Epoca: 81 - Tasa: 0.7 - Error: 0.817757637491794 ## Epoca: 82 - Tasa: 0.7 - Error: 0.817139135402871 ## Epoca: 83 - Tasa: 0.7 - Error: 0.8165212196888 ## Epoca: 84 - Tasa: 0.7 - Error: 0.815903892482576 ## Epoca: 85 - Tasa: 0.7 - Error: 0.815287155895204 ## Epoca: 86 - Tasa: 0.7 - Error: 0.814671012015784 ## Epoca: 87 - Tasa: 0.7 - Error: 0.814055462911596 ## Epoca: 88 - Tasa: 0.7 - Error: 0.813440510628194 ## Epoca: 89 - Tasa: 0.7 - Error: 0.812826157189491 ## Epoca: 90 - Tasa: 0.7 - Error: 0.81221240459785 ## Epoca: 91 - Tasa: 0.7 - Error: 0.811599254834172 ## Epoca: 92 - Tasa: 0.7 - Error: 0.810986709857986 ## Epoca: 93 - Tasa: 0.7 - Error: 0.810374771607539 ## Epoca: 94 - Tasa: 0.7 - Error: 0.809763441999887 ## Epoca: 95 - Tasa: 0.7 - Error: 0.809152722930983 ## Epoca: 96 - Tasa: 0.7 - Error: 0.808542616275769 ## Epoca: 97 - Tasa: 0.7 - Error: 0.807933123888267 ## Epoca: 98 - Tasa: 0.7 - Error: 0.80732424760167 ## Epoca: 99 - Tasa: 0.7 - Error: 0.806715989228429 ## Epoca: 100 - Tasa: 0.7 - Error: 0.806108350560354 ## Epoca: 101 - Tasa: 0.7 - Error: 0.805501333368694 ## Epoca: 102 - Tasa: 0.7 - Error: 0.804894939404236 ## Epoca: 103 - Tasa: 0.7 - Error: 0.804289170397396 ## Epoca: 104 - Tasa: 0.7 - Error: 0.803684028058308 ## Epoca: 105 - Tasa: 0.7 - Error: 0.803079514076919 ## Epoca: 106 - Tasa: 0.7 - Error: 0.80247563012308 ## Epoca: 107 - Tasa: 0.7 - Error: 0.801872377846638 ## Epoca: 108 - Tasa: 0.7 - Error: 0.801269758877532 ## Epoca: 109 - Tasa: 0.7 - Error: 0.80066777482588 ## Epoca: 110 - Tasa: 0.7 - Error: 0.800066427282076 ## Epoca: 111 - Tasa: 0.7 - Error: 0.79946571781688 ## Epoca: 112 - Tasa: 0.7 - Error: 0.798865647981515 ## Epoca: 113 - Tasa: 0.7 - Error: 0.798266219307757 ## Epoca: 114 - Tasa: 0.7 - Error: 0.797667433308026 ## Epoca: 115 - Tasa: 0.7 - Error: 0.797069291475487 ## Epoca: 116 - Tasa: 0.7 - Error: 0.796471795284133 ## Epoca: 117 - Tasa: 0.7 - Error: 0.795874946188888 ## Epoca: 118 - Tasa: 0.7 - Error: 0.795278745625694 ## Epoca: 119 - Tasa: 0.7 - Error: 0.794683195011607 ## Epoca: 120 - Tasa: 0.7 - Error: 0.79408829574489 ## Epoca: 121 - Tasa: 0.7 - Error: 0.793494049205106 ## Epoca: 122 - Tasa: 0.7 - Error: 0.792900456753214 ## Epoca: 123 - Tasa: 0.7 - Error: 0.79230751973166 ## Epoca: 124 - Tasa: 0.7 - Error: 0.79171523946447 ## Epoca: 125 - Tasa: 0.7 - Error: 0.791123617257348 ## Epoca: 126 - Tasa: 0.7 - Error: 0.790532654397764 ## Epoca: 127 - Tasa: 0.7 - Error: 0.789942352155053 ## Epoca: 128 - Tasa: 0.7 - Error: 0.789352711780504 ## Epoca: 129 - Tasa: 0.7 - Error: 0.788763734507458 ## Epoca: 130 - Tasa: 0.7 - Error: 0.788175421551396 ## Epoca: 131 - Tasa: 0.7 - Error: 0.787587774110039 ## Epoca: 132 - Tasa: 0.7 - Error: 0.787000793363438 ## Epoca: 133 - Tasa: 0.7 - Error: 0.786414480474066 ## Epoca: 134 - Tasa: 0.7 - Error: 0.785828836586917 ## Epoca: 135 - Tasa: 0.7 - Error: 0.785243862829593 ## Epoca: 136 - Tasa: 0.7 - Error: 0.784659560312402 ## Epoca: 137 - Tasa: 0.7 - Error: 0.78407593012845 ## Epoca: 138 - Tasa: 0.7 - Error: 0.783492973353732 ## Epoca: 139 - Tasa: 0.7 - Error: 0.78291069104723 ## Epoca: 140 - Tasa: 0.7 - Error: 0.782329084251003 ## Epoca: 141 - Tasa: 0.7 - Error: 0.781748153990279 ## Epoca: 142 - Tasa: 0.7 - Error: 0.781167901273551 ## Epoca: 143 - Tasa: 0.7 - Error: 0.780588327092667 ## Epoca: 144 - Tasa: 0.7 - Error: 0.780009432422927 ## Epoca: 145 - Tasa: 0.7 - Error: 0.77943121822317 ## Epoca: 146 - Tasa: 0.7 - Error: 0.778853685435872 ## Epoca: 147 - Tasa: 0.7 - Error: 0.778276834987235 ## Epoca: 148 - Tasa: 0.7 - Error: 0.777700667787282 ## Epoca: 149 - Tasa: 0.7 - Error: 0.777125184729947 ## Epoca: 150 - Tasa: 0.7 - Error: 0.776550386693167 ## Epoca: 151 - Tasa: 0.7 - Error: 0.775976274538978 ## Epoca: 152 - Tasa: 0.7 - Error: 0.775402849113603 ## Epoca: 153 - Tasa: 0.7 - Error: 0.774830111247544 ## Epoca: 154 - Tasa: 0.7 - Error: 0.774258061755676 ## Epoca: 155 - Tasa: 0.7 - Error: 0.773686701437338 ## Epoca: 156 - Tasa: 0.7 - Error: 0.773116031076423 ## Epoca: 157 - Tasa: 0.7 - Error: 0.772546051441469 ## Epoca: 158 - Tasa: 0.7 - Error: 0.771976763285752 ## Epoca: 159 - Tasa: 0.7 - Error: 0.771408167347377 ## Epoca: 160 - Tasa: 0.7 - Error: 0.770840264349366 ## Epoca: 161 - Tasa: 0.7 - Error: 0.770273054999754 ## Epoca: 162 - Tasa: 0.7 - Error: 0.769706539991672 ## Epoca: 163 - Tasa: 0.7 - Error: 0.769140720003443 ## Epoca: 164 - Tasa: 0.7 - Error: 0.768575595698673 ## Epoca: 165 - Tasa: 0.7 - Error: 0.768011167726334 ## Epoca: 166 - Tasa: 0.7 - Error: 0.767447436720861 ## Epoca: 167 - Tasa: 0.7 - Error: 0.76688440330224 ## Epoca: 168 - Tasa: 0.7 - Error: 0.766322068076094 ## Epoca: 169 - Tasa: 0.7 - Error: 0.765760431633777 ## Epoca: 170 - Tasa: 0.7 - Error: 0.765199494552457 ## Epoca: 171 - Tasa: 0.7 - Error: 0.764639257395213 ## Epoca: 172 - Tasa: 0.7 - Error: 0.764079720711116 ## Epoca: 173 - Tasa: 0.7 - Error: 0.763520885035322 ## Epoca: 174 - Tasa: 0.7 - Error: 0.762962750889159 ## Epoca: 175 - Tasa: 0.7 - Error: 0.762405318780215 ## Epoca: 176 - Tasa: 0.7 - Error: 0.761848589202425 ## Epoca: 177 - Tasa: 0.7 - Error: 0.761292562636159 ## Epoca: 178 - Tasa: 0.7 - Error: 0.760737239548312 ## Epoca: 179 - Tasa: 0.7 - Error: 0.760182620392388 ## Epoca: 180 - Tasa: 0.7 - Error: 0.759628705608588 ## Epoca: 181 - Tasa: 0.7 - Error: 0.759075495623895 ## Epoca: 182 - Tasa: 0.7 - Error: 0.758522990852166 ## Epoca: 183 - Tasa: 0.7 - Error: 0.757971191694211 ## Epoca: 184 - Tasa: 0.7 - Error: 0.757420098537885 ## Epoca: 185 - Tasa: 0.7 - Error: 0.756869711758171 ## Epoca: 186 - Tasa: 0.7 - Error: 0.756320031717266 ## Epoca: 187 - Tasa: 0.7 - Error: 0.755771058764665 ## Epoca: 188 - Tasa: 0.7 - Error: 0.75522279323725 ## Epoca: 189 - Tasa: 0.7 - Error: 0.754675235459371 ## Epoca: 190 - Tasa: 0.7 - Error: 0.754128385742933 ## Epoca: 191 - Tasa: 0.7 - Error: 0.753582244387477 ## Epoca: 192 - Tasa: 0.7 - Error: 0.753036811680271 ## Epoca: 193 - Tasa: 0.7 - Error: 0.752492087896386 ## Epoca: 194 - Tasa: 0.7 - Error: 0.751948073298783 ## Epoca: 195 - Tasa: 0.7 - Error: 0.751404768138398 ## Epoca: 196 - Tasa: 0.7 - Error: 0.750862172654224 ## Epoca: 197 - Tasa: 0.7 - Error: 0.750320287073392 ## Epoca: 198 - Tasa: 0.7 - Error: 0.749779111611255 ## Epoca: 199 - Tasa: 0.7 - Error: 0.749238646471472 ## Epoca: 200 - Tasa: 0.7 - Error: 0.748698891846087 ## Epoca: 1 - Tasa: 0.713333333333333 - Error: 0.862026379792178 ## Epoca: 2 - Tasa: 0.713333333333333 - Error: 0.8269755074123 ## Epoca: 3 - Tasa: 0.713333333333333 - Error: 0.826393578523275 ## Epoca: 4 - Tasa: 0.713333333333333 - Error: 0.82576144050492 ## Epoca: 5 - Tasa: 0.713333333333333 - Error: 0.825129286879211 ## Epoca: 6 - Tasa: 0.713333333333333 - Error: 0.824497582961006 ## Epoca: 7 - Tasa: 0.713333333333333 - Error: 0.823866336645257 ## Epoca: 8 - Tasa: 0.713333333333333 - Error: 0.823235551703389 ## Epoca: 9 - Tasa: 0.713333333333333 - Error: 0.822605231838083 ## Epoca: 10 - Tasa: 0.713333333333333 - Error: 0.821975380719404 ## Epoca: 11 - Tasa: 0.713333333333333 - Error: 0.821346001985235 ## Epoca: 12 - Tasa: 0.713333333333333 - Error: 0.820717099241388 ## Epoca: 13 - Tasa: 0.713333333333333 - Error: 0.820088676061721 ## Epoca: 14 - Tasa: 0.713333333333333 - Error: 0.819460735988253 ## Epoca: 15 - Tasa: 0.713333333333333 - Error: 0.818833282531283 ## Epoca: 16 - Tasa: 0.713333333333333 - Error: 0.8182063191695 ## Epoca: 17 - Tasa: 0.713333333333333 - Error: 0.817579849350108 ## Epoca: 18 - Tasa: 0.713333333333333 - Error: 0.816953876488943 ## Epoca: 19 - Tasa: 0.713333333333333 - Error: 0.816328403970591 ## Epoca: 20 - Tasa: 0.713333333333333 - Error: 0.815703435148508 ## Epoca: 21 - Tasa: 0.713333333333333 - Error: 0.815078973345145 ## Epoca: 22 - Tasa: 0.713333333333333 - Error: 0.814455021852068 ## Epoca: 23 - Tasa: 0.713333333333333 - Error: 0.81383158393008 ## Epoca: 24 - Tasa: 0.713333333333333 - Error: 0.813208662809346 ## Epoca: 25 - Tasa: 0.713333333333333 - Error: 0.812586261689515 ## Epoca: 26 - Tasa: 0.713333333333333 - Error: 0.811964383739847 ## Epoca: 27 - Tasa: 0.713333333333333 - Error: 0.81134303209934 ## Epoca: 28 - Tasa: 0.713333333333333 - Error: 0.810722209876851 ## Epoca: 29 - Tasa: 0.713333333333333 - Error: 0.810101920151226 ## Epoca: 30 - Tasa: 0.713333333333333 - Error: 0.80948216597143 ## Epoca: 31 - Tasa: 0.713333333333333 - Error: 0.808862950356666 ## Epoca: 32 - Tasa: 0.713333333333333 - Error: 0.808244276296516 ## Epoca: 33 - Tasa: 0.713333333333333 - Error: 0.807626146751057 ## Epoca: 34 - Tasa: 0.713333333333333 - Error: 0.807008564650998 ## Epoca: 35 - Tasa: 0.713333333333333 - Error: 0.806391532897811 ## Epoca: 36 - Tasa: 0.713333333333333 - Error: 0.805775054363853 ## Epoca: 37 - Tasa: 0.713333333333333 - Error: 0.805159131892505 ## Epoca: 38 - Tasa: 0.713333333333333 - Error: 0.804543768298299 ## Epoca: 39 - Tasa: 0.713333333333333 - Error: 0.803928966367051 ## Epoca: 40 - Tasa: 0.713333333333333 - Error: 0.80331472885599 ## Epoca: 41 - Tasa: 0.713333333333333 - Error: 0.802701058493895 ## Epoca: 42 - Tasa: 0.713333333333333 - Error: 0.802087957981221 ## Epoca: 43 - Tasa: 0.713333333333333 - Error: 0.801475429990239 ## Epoca: 44 - Tasa: 0.713333333333333 - Error: 0.800863477165163 ## Epoca: 45 - Tasa: 0.713333333333333 - Error: 0.800252102122285 ## Epoca: 46 - Tasa: 0.713333333333333 - Error: 0.799641307450111 ## Epoca: 47 - Tasa: 0.713333333333333 - Error: 0.799031095709491 ## Epoca: 48 - Tasa: 0.713333333333333 - Error: 0.798421469433758 ## Epoca: 49 - Tasa: 0.713333333333333 - Error: 0.797812431128857 ## Epoca: 50 - Tasa: 0.713333333333333 - Error: 0.797203983273481 ## Epoca: 51 - Tasa: 0.713333333333333 - Error: 0.796596128319206 ## Epoca: 52 - Tasa: 0.713333333333333 - Error: 0.79598886869063 ## Epoca: 53 - Tasa: 0.713333333333333 - Error: 0.795382206785498 ## Epoca: 54 - Tasa: 0.713333333333333 - Error: 0.794776144974846 ## Epoca: 55 - Tasa: 0.713333333333333 - Error: 0.794170685603133 ## Epoca: 56 - Tasa: 0.713333333333333 - Error: 0.793565830988376 ## Epoca: 57 - Tasa: 0.713333333333333 - Error: 0.792961583422283 ## Epoca: 58 - Tasa: 0.713333333333333 - Error: 0.792357945170394 ## Epoca: 59 - Tasa: 0.713333333333333 - Error: 0.791754918472211 ## Epoca: 60 - Tasa: 0.713333333333333 - Error: 0.791152505541338 ## Epoca: 61 - Tasa: 0.713333333333333 - Error: 0.79055070856561 ## Epoca: 62 - Tasa: 0.713333333333333 - Error: 0.789949529707238 ## Epoca: 63 - Tasa: 0.713333333333333 - Error: 0.789348971102935 ## Epoca: 64 - Tasa: 0.713333333333333 - Error: 0.788749034864058 ## Epoca: 65 - Tasa: 0.713333333333333 - Error: 0.78814972307674 ## Epoca: 66 - Tasa: 0.713333333333333 - Error: 0.787551037802028 ## Epoca: 67 - Tasa: 0.713333333333333 - Error: 0.786952981076015 ## Epoca: 68 - Tasa: 0.713333333333333 - Error: 0.78635555490998 ## Epoca: 69 - Tasa: 0.713333333333333 - Error: 0.78575876129052 ## Epoca: 70 - Tasa: 0.713333333333333 - Error: 0.785162602179684 ## Epoca: 71 - Tasa: 0.713333333333333 - Error: 0.784567079515113 ## Epoca: 72 - Tasa: 0.713333333333333 - Error: 0.783972195210172 ## Epoca: 73 - Tasa: 0.713333333333333 - Error: 0.783377951154082 ## Epoca: 74 - Tasa: 0.713333333333333 - Error: 0.782784349212063 ## Epoca: 75 - Tasa: 0.713333333333333 - Error: 0.782191391225459 ## Epoca: 76 - Tasa: 0.713333333333333 - Error: 0.781599079011881 ## Epoca: 77 - Tasa: 0.713333333333333 - Error: 0.781007414365335 ## Epoca: 78 - Tasa: 0.713333333333333 - Error: 0.780416399056361 ## Epoca: 79 - Tasa: 0.713333333333333 - Error: 0.779826034832163 ## Epoca: 80 - Tasa: 0.713333333333333 - Error: 0.779236323416747 ## Epoca: 81 - Tasa: 0.713333333333333 - Error: 0.778647266511051 ## Epoca: 82 - Tasa: 0.713333333333333 - Error: 0.778058865793083 ## Epoca: 83 - Tasa: 0.713333333333333 - Error: 0.777471122918049 ## Epoca: 84 - Tasa: 0.713333333333333 - Error: 0.77688403951849 ## Epoca: 85 - Tasa: 0.713333333333333 - Error: 0.776297617204413 ## Epoca: 86 - Tasa: 0.713333333333333 - Error: 0.775711857563426 ## Epoca: 87 - Tasa: 0.713333333333333 - Error: 0.775126762160868 ## Epoca: 88 - Tasa: 0.713333333333333 - Error: 0.774542332539942 ## Epoca: 89 - Tasa: 0.713333333333333 - Error: 0.773958570221848 ## Epoca: 90 - Tasa: 0.713333333333333 - Error: 0.773375476705913 ## Epoca: 91 - Tasa: 0.713333333333333 - Error: 0.772793053469725 ## Epoca: 92 - Tasa: 0.713333333333333 - Error: 0.772211301969262 ## Epoca: 93 - Tasa: 0.713333333333333 - Error: 0.771630223639024 ## Epoca: 94 - Tasa: 0.713333333333333 - Error: 0.771049819892163 ## Epoca: 95 - Tasa: 0.713333333333333 - Error: 0.770470092120614 ## Epoca: 96 - Tasa: 0.713333333333333 - Error: 0.769891041695224 ## Epoca: 97 - Tasa: 0.713333333333333 - Error: 0.769312669965886 ## Epoca: 98 - Tasa: 0.713333333333333 - Error: 0.768734978261662 ## Epoca: 99 - Tasa: 0.713333333333333 - Error: 0.768157967890916 ## Epoca: 100 - Tasa: 0.713333333333333 - Error: 0.767581640141444 ## Epoca: 101 - Tasa: 0.713333333333333 - Error: 0.7670059962806 ## Epoca: 102 - Tasa: 0.713333333333333 - Error: 0.766431037555426 ## Epoca: 103 - Tasa: 0.713333333333333 - Error: 0.765856765192777 ## Epoca: 104 - Tasa: 0.713333333333333 - Error: 0.765283180399453 ## Epoca: 105 - Tasa: 0.713333333333333 - Error: 0.764710284362323 ## Epoca: 106 - Tasa: 0.713333333333333 - Error: 0.764138078248454 ## Epoca: 107 - Tasa: 0.713333333333333 - Error: 0.763566563205235 ## Epoca: 108 - Tasa: 0.713333333333333 - Error: 0.762995740360506 ## Epoca: 109 - Tasa: 0.713333333333333 - Error: 0.762425610822683 ## Epoca: 110 - Tasa: 0.713333333333333 - Error: 0.76185617568088 ## Epoca: 111 - Tasa: 0.713333333333333 - Error: 0.761287436005041 ## Epoca: 112 - Tasa: 0.713333333333333 - Error: 0.760719392846059 ## Epoca: 113 - Tasa: 0.713333333333333 - Error: 0.760152047235902 ## Epoca: 114 - Tasa: 0.713333333333333 - Error: 0.759585400187738 ## Epoca: 115 - Tasa: 0.713333333333333 - Error: 0.759019452696058 ## Epoca: 116 - Tasa: 0.713333333333333 - Error: 0.758454205736799 ## Epoca: 117 - Tasa: 0.713333333333333 - Error: 0.757889660267468 ## Epoca: 118 - Tasa: 0.713333333333333 - Error: 0.75732581722726 ## Epoca: 119 - Tasa: 0.713333333333333 - Error: 0.756762677537189 ## Epoca: 120 - Tasa: 0.713333333333333 - Error: 0.756200242100199 ## Epoca: 121 - Tasa: 0.713333333333333 - Error: 0.755638511801294 ## Epoca: 122 - Tasa: 0.713333333333333 - Error: 0.755077487507653 ## Epoca: 123 - Tasa: 0.713333333333333 - Error: 0.754517170068754 ## Epoca: 124 - Tasa: 0.713333333333333 - Error: 0.753957560316494 ## Epoca: 125 - Tasa: 0.713333333333333 - Error: 0.753398659065303 ## Epoca: 126 - Tasa: 0.713333333333333 - Error: 0.752840467112273 ## Epoca: 127 - Tasa: 0.713333333333333 - Error: 0.752282985237268 ## Epoca: 128 - Tasa: 0.713333333333333 - Error: 0.751726214203047 ## Epoca: 129 - Tasa: 0.713333333333333 - Error: 0.75117015475538 ## Epoca: 130 - Tasa: 0.713333333333333 - Error: 0.750614807623166 ## Epoca: 131 - Tasa: 0.713333333333333 - Error: 0.750060173518551 ## Epoca: 132 - Tasa: 0.713333333333333 - Error: 0.749506253137045 ## Epoca: 133 - Tasa: 0.713333333333333 - Error: 0.748953047157634 ## Epoca: 134 - Tasa: 0.713333333333333 - Error: 0.748400556242901 ## Epoca: 135 - Tasa: 0.713333333333333 - Error: 0.74784878103914 ## Epoca: 136 - Tasa: 0.713333333333333 - Error: 0.747297722176469 ## Epoca: 137 - Tasa: 0.713333333333333 - Error: 0.746747380268944 ## Epoca: 138 - Tasa: 0.713333333333333 - Error: 0.746197755914679 ## Epoca: 139 - Tasa: 0.713333333333333 - Error: 0.745648849695952 ## Epoca: 140 - Tasa: 0.713333333333333 - Error: 0.74510066217932 ## Epoca: 141 - Tasa: 0.713333333333333 - Error: 0.744553193915738 ## Epoca: 142 - Tasa: 0.713333333333333 - Error: 0.744006445440661 ## Epoca: 143 - Tasa: 0.713333333333333 - Error: 0.743460417274165 ## Epoca: 144 - Tasa: 0.713333333333333 - Error: 0.742915109921051 ## Epoca: 145 - Tasa: 0.713333333333333 - Error: 0.742370523870963 ## Epoca: 146 - Tasa: 0.713333333333333 - Error: 0.741826659598492 ## Epoca: 147 - Tasa: 0.713333333333333 - Error: 0.741283517563288 ## Epoca: 148 - Tasa: 0.713333333333333 - Error: 0.740741098210174 ## Epoca: 149 - Tasa: 0.713333333333333 - Error: 0.740199401969246 ## Epoca: 150 - Tasa: 0.713333333333333 - Error: 0.739658429255992 ## Epoca: 151 - Tasa: 0.713333333333333 - Error: 0.739118180471391 ## Epoca: 152 - Tasa: 0.713333333333333 - Error: 0.738578656002027 ## Epoca: 153 - Tasa: 0.713333333333333 - Error: 0.738039856220191 ## Epoca: 154 - Tasa: 0.713333333333333 - Error: 0.737501781483995 ## Epoca: 155 - Tasa: 0.713333333333333 - Error: 0.736964432137469 ## Epoca: 156 - Tasa: 0.713333333333333 - Error: 0.736427808510676 ## Epoca: 157 - Tasa: 0.713333333333333 - Error: 0.735891910919809 ## Epoca: 158 - Tasa: 0.713333333333333 - Error: 0.735356739667303 ## Epoca: 159 - Tasa: 0.713333333333333 - Error: 0.734822295041935 ## Epoca: 160 - Tasa: 0.713333333333333 - Error: 0.73428857731893 ## Epoca: 161 - Tasa: 0.713333333333333 - Error: 0.733755586760062 ## Epoca: 162 - Tasa: 0.713333333333333 - Error: 0.733223323613761 ## Epoca: 163 - Tasa: 0.713333333333333 - Error: 0.732691788115212 ## Epoca: 164 - Tasa: 0.713333333333333 - Error: 0.732160980486458 ## Epoca: 165 - Tasa: 0.713333333333333 - Error: 0.731630900936505 ## Epoca: 166 - Tasa: 0.713333333333333 - Error: 0.731101549661417 ## Epoca: 167 - Tasa: 0.713333333333333 - Error: 0.73057292684442 ## Epoca: 168 - Tasa: 0.713333333333333 - Error: 0.730045032656005 ## Epoca: 169 - Tasa: 0.713333333333333 - Error: 0.729517867254021 ## Epoca: 170 - Tasa: 0.713333333333333 - Error: 0.728991430783781 ## Epoca: 171 - Tasa: 0.713333333333333 - Error: 0.728465723378157 ## Epoca: 172 - Tasa: 0.713333333333333 - Error: 0.727940745157679 ## Epoca: 173 - Tasa: 0.713333333333333 - Error: 0.727416496230634 ## Epoca: 174 - Tasa: 0.713333333333333 - Error: 0.726892976693161 ## Epoca: 175 - Tasa: 0.713333333333333 - Error: 0.72637018662935 ## Epoca: 176 - Tasa: 0.713333333333333 - Error: 0.725848126111339 ## Epoca: 177 - Tasa: 0.713333333333333 - Error: 0.725326795199407 ## Epoca: 178 - Tasa: 0.713333333333333 - Error: 0.724806193942074 ## Epoca: 179 - Tasa: 0.713333333333333 - Error: 0.724286322376191 ## Epoca: 180 - Tasa: 0.713333333333333 - Error: 0.723767180527039 ## Epoca: 181 - Tasa: 0.713333333333333 - Error: 0.723248768408419 ## Epoca: 182 - Tasa: 0.713333333333333 - Error: 0.722731086022749 ## Epoca: 183 - Tasa: 0.713333333333333 - Error: 0.722214133361156 ## Epoca: 184 - Tasa: 0.713333333333333 - Error: 0.721697910403569 ## Epoca: 185 - Tasa: 0.713333333333333 - Error: 0.721182417118811 ## Epoca: 186 - Tasa: 0.713333333333333 - Error: 0.720667653464691 ## Epoca: 187 - Tasa: 0.713333333333333 - Error: 0.720153619388092 ## Epoca: 188 - Tasa: 0.713333333333333 - Error: 0.71964031482507 ## Epoca: 189 - Tasa: 0.713333333333333 - Error: 0.719127739700935 ## Epoca: 190 - Tasa: 0.713333333333333 - Error: 0.718615893930347 ## Epoca: 191 - Tasa: 0.713333333333333 - Error: 0.718104777417404 ## Epoca: 192 - Tasa: 0.713333333333333 - Error: 0.717594390055731 ## Epoca: 193 - Tasa: 0.713333333333333 - Error: 0.717084731728566 ## Epoca: 194 - Tasa: 0.713333333333333 - Error: 0.716575802308852 ## Epoca: 195 - Tasa: 0.713333333333333 - Error: 0.716067601659324 ## Epoca: 196 - Tasa: 0.713333333333333 - Error: 0.715560129632592 ## Epoca: 197 - Tasa: 0.713333333333333 - Error: 0.715053386071233 ## Epoca: 198 - Tasa: 0.713333333333333 - Error: 0.714547370807874 ## Epoca: 199 - Tasa: 0.713333333333333 - Error: 0.71404208366528 ## Epoca: 200 - Tasa: 0.713333333333333 - Error: 0.713537524456436 ## Epoca: 1 - Tasa: 0.586666666666667 - Error: 0.996073686376647 ## Epoca: 2 - Tasa: 0.586666666666667 - Error: 0.984882213613751 ## Epoca: 3 - Tasa: 0.586666666666667 - Error: 0.982225166824311 ## Epoca: 4 - Tasa: 0.586666666666667 - Error: 0.979563249371207 ## Epoca: 5 - Tasa: 0.586666666666667 - Error: 0.976906672452261 ## Epoca: 6 - Tasa: 0.586666666666667 - Error: 0.974255491693144 ## Epoca: 7 - Tasa: 0.586666666666667 - Error: 0.971609749611466 ## Epoca: 8 - Tasa: 0.586666666666667 - Error: 0.968969487824729 ## Epoca: 9 - Tasa: 0.586666666666667 - Error: 0.966334747075322 ## Epoca: 10 - Tasa: 0.586666666666667 - Error: 0.963705567241068 ## Epoca: 11 - Tasa: 0.586666666666667 - Error: 0.961081987345632 ## Epoca: 12 - Tasa: 0.586666666666667 - Error: 0.958464045568792 ## Epoca: 13 - Tasa: 0.586666666666667 - Error: 0.955851779256586 ## Epoca: 14 - Tasa: 0.586666666666667 - Error: 0.953245224931337 ## Epoca: 15 - Tasa: 0.586666666666667 - Error: 0.950644418301546 ## Epoca: 16 - Tasa: 0.586666666666667 - Error: 0.948049394271675 ## Epoca: 17 - Tasa: 0.586666666666667 - Error: 0.945460186951802 ## Epoca: 18 - Tasa: 0.586666666666667 - Error: 0.942876829667166 ## Epoca: 19 - Tasa: 0.586666666666667 - Error: 0.940299354967594 ## Epoca: 20 - Tasa: 0.586666666666667 - Error: 0.937727794636819 ## Epoca: 21 - Tasa: 0.586666666666667 - Error: 0.935162179701685 ## Epoca: 22 - Tasa: 0.586666666666667 - Error: 0.932602540441243 ## Epoca: 23 - Tasa: 0.586666666666667 - Error: 0.930048906395745 ## Epoca: 24 - Tasa: 0.586666666666667 - Error: 0.92750130637553 ## Epoca: 25 - Tasa: 0.586666666666667 - Error: 0.924959768469808 ## Epoca: 26 - Tasa: 0.586666666666667 - Error: 0.922424320055344 ## Epoca: 27 - Tasa: 0.586666666666667 - Error: 0.919894987805046 ## Epoca: 28 - Tasa: 0.586666666666667 - Error: 0.917371797696448 ## Epoca: 29 - Tasa: 0.586666666666667 - Error: 0.914854775020109 ## Epoca: 30 - Tasa: 0.586666666666667 - Error: 0.91234394438791 ## Epoca: 31 - Tasa: 0.586666666666667 - Error: 0.909839329741259 ## Epoca: 32 - Tasa: 0.586666666666667 - Error: 0.907340954359215 ## Epoca: 33 - Tasa: 0.586666666666667 - Error: 0.90484884086651 ## Epoca: 34 - Tasa: 0.586666666666667 - Error: 0.902363011241498 ## Epoca: 35 - Tasa: 0.586666666666667 - Error: 0.899883486824005 ## Epoca: 36 - Tasa: 0.586666666666667 - Error: 0.897410288323104 ## Epoca: 37 - Tasa: 0.586666666666667 - Error: 0.894943435824807 ## Epoca: 38 - Tasa: 0.586666666666667 - Error: 0.892482948799669 ## Epoca: 39 - Tasa: 0.586666666666667 - Error: 0.890028846110321 ## Epoca: 40 - Tasa: 0.586666666666667 - Error: 0.887581146018918 ## Epoca: 41 - Tasa: 0.586666666666667 - Error: 0.885139866194512 ## Epoca: 42 - Tasa: 0.586666666666667 - Error: 0.882705023720353 ## Epoca: 43 - Tasa: 0.586666666666667 - Error: 0.880276635101105 ## Epoca: 44 - Tasa: 0.586666666666667 - Error: 0.87785471627 ## Epoca: 45 - Tasa: 0.586666666666667 - Error: 0.875439282595915 ## Epoca: 46 - Tasa: 0.586666666666667 - Error: 0.873030348890374 ## Epoca: 47 - Tasa: 0.586666666666667 - Error: 0.87062792941449 ## Epoca: 48 - Tasa: 0.586666666666667 - Error: 0.868232037885823 ## Epoca: 49 - Tasa: 0.586666666666667 - Error: 0.865842687485188 ## Epoca: 50 - Tasa: 0.586666666666667 - Error: 0.863459890863379 ## Epoca: 51 - Tasa: 0.586666666666667 - Error: 0.861083660147842 ## Epoca: 52 - Tasa: 0.586666666666667 - Error: 0.858714006949271 ## Epoca: 53 - Tasa: 0.586666666666667 - Error: 0.856350942368146 ## Epoca: 54 - Tasa: 0.586666666666667 - Error: 0.85399447700121 ## Epoca: 55 - Tasa: 0.586666666666667 - Error: 0.851644620947876 ## Epoca: 56 - Tasa: 0.586666666666667 - Error: 0.849301383816581 ## Epoca: 57 - Tasa: 0.586666666666667 - Error: 0.846964774731075 ## Epoca: 58 - Tasa: 0.586666666666667 - Error: 0.844634802336649 ## Epoca: 59 - Tasa: 0.586666666666667 - Error: 0.842311474806308 ## Epoca: 60 - Tasa: 0.586666666666667 - Error: 0.839994799846885 ## Epoca: 61 - Tasa: 0.586666666666667 - Error: 0.837684784705091 ## Epoca: 62 - Tasa: 0.586666666666667 - Error: 0.835381436173521 ## Epoca: 63 - Tasa: 0.586666666666667 - Error: 0.833084760596589 ## Epoca: 64 - Tasa: 0.586666666666667 - Error: 0.830794763876422 ## Epoca: 65 - Tasa: 0.586666666666667 - Error: 0.828511451478687 ## Epoca: 66 - Tasa: 0.586666666666667 - Error: 0.82623482843837 ## Epoca: 67 - Tasa: 0.586666666666667 - Error: 0.823964899365503 ## Epoca: 68 - Tasa: 0.586666666666667 - Error: 0.821701668450836 ## Epoca: 69 - Tasa: 0.586666666666667 - Error: 0.819445139471452 ## Epoca: 70 - Tasa: 0.586666666666667 - Error: 0.817195315796339 ## Epoca: 71 - Tasa: 0.586666666666667 - Error: 0.814952200391903 ## Epoca: 72 - Tasa: 0.586666666666667 - Error: 0.812715795827437 ## Epoca: 73 - Tasa: 0.586666666666667 - Error: 0.810486104280534 ## Epoca: 74 - Tasa: 0.586666666666667 - Error: 0.808263127542453 ## Epoca: 75 - Tasa: 0.586666666666667 - Error: 0.806046867023438 ## Epoca: 76 - Tasa: 0.586666666666667 - Error: 0.803837323757981 ## Epoca: 77 - Tasa: 0.586666666666667 - Error: 0.801634498410046 ## Epoca: 78 - Tasa: 0.586666666666667 - Error: 0.799438391278239 ## Epoca: 79 - Tasa: 0.586666666666667 - Error: 0.79724900230093 ## Epoca: 80 - Tasa: 0.586666666666667 - Error: 0.795066331061334 ## Epoca: 81 - Tasa: 0.586666666666667 - Error: 0.792890376792541 ## Epoca: 82 - Tasa: 0.586666666666667 - Error: 0.790721138382498 ## Epoca: 83 - Tasa: 0.586666666666667 - Error: 0.788558614378949 ## Epoca: 84 - Tasa: 0.586666666666667 - Error: 0.78640280299433 ## Epoca: 85 - Tasa: 0.586666666666667 - Error: 0.784253702110614 ## Epoca: 86 - Tasa: 0.586666666666667 - Error: 0.782111309284117 ## Epoca: 87 - Tasa: 0.586666666666667 - Error: 0.779975621750258 ## Epoca: 88 - Tasa: 0.586666666666667 - Error: 0.777846636428269 ## Epoca: 89 - Tasa: 0.586666666666667 - Error: 0.775724349925874 ## Epoca: 90 - Tasa: 0.586666666666667 - Error: 0.773608758543913 ## Epoca: 91 - Tasa: 0.586666666666667 - Error: 0.771499858280926 ## Epoca: 92 - Tasa: 0.586666666666667 - Error: 0.769397644837698 ## Epoca: 93 - Tasa: 0.586666666666667 - Error: 0.767302113621761 ## Epoca: 94 - Tasa: 0.586666666666667 - Error: 0.765213259751845 ## Epoca: 95 - Tasa: 0.586666666666667 - Error: 0.763131078062303 ## Epoca: 96 - Tasa: 0.586666666666667 - Error: 0.76105556310748 ## Epoca: 97 - Tasa: 0.586666666666667 - Error: 0.758986709166047 ## Epoca: 98 - Tasa: 0.586666666666667 - Error: 0.756924510245297 ## Epoca: 99 - Tasa: 0.586666666666667 - Error: 0.754868960085392 ## Epoca: 100 - Tasa: 0.586666666666667 - Error: 0.752820052163581 ## Epoca: 101 - Tasa: 0.586666666666667 - Error: 0.750777779698362 ## Epoca: 102 - Tasa: 0.586666666666667 - Error: 0.748742135653624 ## Epoca: 103 - Tasa: 0.586666666666667 - Error: 0.746713112742731 ## Epoca: 104 - Tasa: 0.586666666666667 - Error: 0.744690703432578 ## Epoca: 105 - Tasa: 0.586666666666667 - Error: 0.742674899947601 ## Epoca: 106 - Tasa: 0.586666666666667 - Error: 0.740665694273758 ## Epoca: 107 - Tasa: 0.586666666666667 - Error: 0.738663078162458 ## Epoca: 108 - Tasa: 0.586666666666667 - Error: 0.73666704313446 ## Epoca: 109 - Tasa: 0.586666666666667 - Error: 0.734677580483735 ## Epoca: 110 - Tasa: 0.586666666666667 - Error: 0.732694681281283 ## Epoca: 111 - Tasa: 0.586666666666667 - Error: 0.730718336378921 ## Epoca: 112 - Tasa: 0.586666666666667 - Error: 0.728748536413024 ## Epoca: 113 - Tasa: 0.586666666666667 - Error: 0.726785271808239 ## Epoca: 114 - Tasa: 0.586666666666667 - Error: 0.724828532781152 ## Epoca: 115 - Tasa: 0.586666666666667 - Error: 0.722878309343924 ## Epoca: 116 - Tasa: 0.586666666666667 - Error: 0.720934591307893 ## Epoca: 117 - Tasa: 0.586666666666667 - Error: 0.71899736828713 ## Epoca: 118 - Tasa: 0.586666666666667 - Error: 0.717066629701969 ## Epoca: 119 - Tasa: 0.586666666666667 - Error: 0.715142364782495 ## Epoca: 120 - Tasa: 0.586666666666667 - Error: 0.713224562571996 ## Epoca: 121 - Tasa: 0.586666666666667 - Error: 0.711313211930388 ## Epoca: 122 - Tasa: 0.586666666666667 - Error: 0.709408301537589 ## Epoca: 123 - Tasa: 0.586666666666667 - Error: 0.707509819896876 ## Epoca: 124 - Tasa: 0.586666666666667 - Error: 0.705617755338191 ## Epoca: 125 - Tasa: 0.586666666666667 - Error: 0.703732096021424 ## Epoca: 126 - Tasa: 0.586666666666667 - Error: 0.701852829939654 ## Epoca: 127 - Tasa: 0.586666666666667 - Error: 0.69997994492236 ## Epoca: 128 - Tasa: 0.586666666666667 - Error: 0.698113428638598 ## Epoca: 129 - Tasa: 0.586666666666667 - Error: 0.696253268600141 ## Epoca: 130 - Tasa: 0.586666666666667 - Error: 0.694399452164586 ## Epoca: 131 - Tasa: 0.586666666666667 - Error: 0.692551966538431 ## Epoca: 132 - Tasa: 0.586666666666667 - Error: 0.690710798780117 ## Epoca: 133 - Tasa: 0.586666666666667 - Error: 0.688875935803033 ## Epoca: 134 - Tasa: 0.586666666666667 - Error: 0.687047364378494 ## Epoca: 135 - Tasa: 0.586666666666667 - Error: 0.685225071138683 ## Epoca: 136 - Tasa: 0.586666666666667 - Error: 0.683409042579563 ## Epoca: 137 - Tasa: 0.586666666666667 - Error: 0.681599265063753 ## Epoca: 138 - Tasa: 0.586666666666667 - Error: 0.679795724823375 ## Epoca: 139 - Tasa: 0.586666666666667 - Error: 0.67799840796287 ## Epoca: 140 - Tasa: 0.586666666666667 - Error: 0.67620730046178 ## Epoca: 141 - Tasa: 0.586666666666667 - Error: 0.674422388177496 ## Epoca: 142 - Tasa: 0.586666666666667 - Error: 0.672643656847984 ## Epoca: 143 - Tasa: 0.64 - Error: 0.670871092094471 ## Epoca: 144 - Tasa: 0.686666666666667 - Error: 0.669104679424103 ## Epoca: 145 - Tasa: 0.686666666666667 - Error: 0.667344404232573 ## Epoca: 146 - Tasa: 0.686666666666667 - Error: 0.66559025180672 ## Epoca: 147 - Tasa: 0.686666666666667 - Error: 0.663842207327096 ## Epoca: 148 - Tasa: 0.686666666666667 - Error: 0.662100255870499 ## Epoca: 149 - Tasa: 0.686666666666667 - Error: 0.660364382412489 ## Epoca: 150 - Tasa: 0.713333333333333 - Error: 0.658634571829857 ## Epoca: 151 - Tasa: 0.72 - Error: 0.656910808903078 ## Epoca: 152 - Tasa: 0.72 - Error: 0.65519307831873 ## Epoca: 153 - Tasa: 0.76 - Error: 0.653481364671884 ## Epoca: 154 - Tasa: 0.76 - Error: 0.651775652468466 ## Epoca: 155 - Tasa: 0.76 - Error: 0.650075926127589 ## Epoca: 156 - Tasa: 0.773333333333333 - Error: 0.648382169983858 ## Epoca: 157 - Tasa: 0.773333333333333 - Error: 0.646694368289649 ## Epoca: 158 - Tasa: 0.773333333333333 - Error: 0.645012505217353 ## Epoca: 159 - Tasa: 0.786666666666667 - Error: 0.643336564861601 ## Epoca: 160 - Tasa: 0.84 - Error: 0.641666531241456 ## Epoca: 161 - Tasa: 0.84 - Error: 0.640002388302578 ## Epoca: 162 - Tasa: 0.84 - Error: 0.638344119919365 ## Epoca: 163 - Tasa: 0.84 - Error: 0.636691709897061 ## Epoca: 164 - Tasa: 0.84 - Error: 0.635045141973846 ## Epoca: 165 - Tasa: 0.84 - Error: 0.633404399822894 ## Epoca: 166 - Tasa: 0.84 - Error: 0.6317694670544 ## Epoca: 167 - Tasa: 0.84 - Error: 0.630140327217595 ## Epoca: 168 - Tasa: 0.84 - Error: 0.62851696380272 ## Epoca: 169 - Tasa: 0.84 - Error: 0.626899360242981 ## Epoca: 170 - Tasa: 0.846666666666667 - Error: 0.625287499916482 ## Epoca: 171 - Tasa: 0.873333333333333 - Error: 0.623681366148127 ## Epoca: 172 - Tasa: 0.873333333333333 - Error: 0.622080942211497 ## Epoca: 173 - Tasa: 0.9 - Error: 0.620486211330707 ## Epoca: 174 - Tasa: 0.9 - Error: 0.618897156682233 ## Epoca: 175 - Tasa: 0.9 - Error: 0.617313761396719 ## Epoca: 176 - Tasa: 0.9 - Error: 0.615736008560756 ## Epoca: 177 - Tasa: 0.9 - Error: 0.61416388121864 ## Epoca: 178 - Tasa: 0.9 - Error: 0.6125973623741 ## Epoca: 179 - Tasa: 0.9 - Error: 0.611036434992016 ## Epoca: 180 - Tasa: 0.906666666666667 - Error: 0.609481082000094 Prueba con datos salida_2 &lt;- aplicarRedRBF(modeloRBF_2, datos_x, datos_y) salida_2$tasa ## [1] 0.7133333 head(salida_2$salida) ## 1 1 1 ## 1 -1 -1 1 ## 2 -1 -1 1 ## 3 1 -1 -1 ## 4 -1 -1 1 ## 5 1 -1 -1 ## 6 1 -1 -1 2.2 Ejercicio 2 Lectura de datos merval &lt;- read_csv(&quot;../../PUBLICO/Encuentro 3/Práctica/data/merval.csv&quot;, col_names = FALSE) Preprocesamiento de los datos Generamos un dataset que contenga seis valores consecutivos en cada registro, cinco tomados como datos de entrada y un sexto valor tomado como clase. cantidadDatos &lt;- nrow(merval) datos_merval &lt;- matrix(0,nrow = cantidadDatos-5, ncol = 6) for (i in seq(1,cantidadDatos-5)) { datos_merval[i,] &lt;- merval$X1[seq(i,i+5)] } datos_x &lt;- datos_merval[,c(1,2,3,4,5)] %&gt;% as.matrix() datos_y &lt;- datos_merval[,6] %&gt;% as.matrix() # Primeros seis registros del dataset head(datos_merval) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 215 212 229 253 254 235 ## [2,] 212 229 253 254 235 239 ## [3,] 229 253 254 235 239 241 ## [4,] 253 254 235 239 241 252 ## [5,] 254 235 239 241 252 253 ## [6,] 235 239 241 252 253 257 Antes de generar el modelo, tenemos que definir el número de gausianas. Utilizamos la gráfica de Elbow para definir el k a utilizar en el modelo. set.seed(123) wss &lt;- function(k) { kmeans(datos_x, k, nstart = 10 )$tot.withinss } # Valores de k = 1 a k = 15 k.values &lt;- 1:15 wss_values &lt;- map_dbl(k.values, wss) plot(k.values, wss_values, type=&quot;b&quot;, pch = 19, frame = FALSE, xlab=&quot;Número de clusters K&quot;, ylab=&quot;Inercia - Suma de Cuadrados&quot;) Mirando la gráfica anterior tomamos un valor de k = 4, es donde la gráfica hace el codo y queda aproximadamente constante. Normalizamos los datos. maximo &lt;- 0 for (i in seq(1,ncol(datos_x))) { if (max(datos_x[,i]) &gt; maximo) {maximo &lt;- max(datos_x[,i])} } if (max(datos_y) &gt; maximo) {maximo &lt;- max(datos_y)} datos_x &lt;- datos_x / maximo datos_y &lt;- datos_y / maximo Dividimos los datos en Train y Test, utilizando un 70% para entrenamiento. merval7030 &lt;- generarParticionPorID(as.data.frame(datos_merval), porcEntrenamiento = 0.7, semilla = 1, clase = &quot;V6&quot;) Generamos el modelo con los datos de entrenamiento. if (calcular) { modeloMerval70 &lt;- redRBF(datos_x[merval7030$trn,], (datos_y[merval7030$trn,] %&gt;% as.matrix()), nroGausianas = 4, funcion = &quot;lineal&quot;, pnu = 0.01, pepoca = 200000, pcritFinalizacion = 0.95, ptolerencia = 0.1) } Aplicamos el modelo a los datos de Train y Test salidaMervalTrn &lt;- aplicarRedRBF(modeloMerval70, datos_x[merval7030$trn,], (datos_y[merval7030$trn,] %&gt;% as.matrix())) salidaMervalTst &lt;- aplicarRedRBF(modeloMerval70, datos_x[merval7030$tst,], (datos_y[merval7030$tst,] %&gt;% as.matrix())) Grafica de error en Test #Grafica de error en cada registro plot(salidaMervalTst$error) #Error cuadrático medio errorMedio &lt;- sum(salidaMervalTst$error) / length(salidaMervalTst$error) errorMedio ## [1] 0.001068555 Generamos el modelo a aplicar para realizar las predicciones con todos los datos. # Generamos el modelo if (calcular) { modeloMerval &lt;- redRBF(datos_x, datos_y, nroGausianas = 4, funcion = &quot;lineal&quot;, pnu = 0.01, pepoca = 200000, pcritFinalizacion = 0.95, ptolerencia = 0.1) } Aplicamos el modelo a los mismos datos de entrenamiento para graficar error en train. salidaMerval &lt;- aplicarRedRBF(modeloMerval, datos_x, datos_y) Grafica de error #Grafica de error en cada registro plot(salidaMerval$error) #Error cuadrático medio errorMedio &lt;- sum(salidaMerval$error) / length(salidaMerval$error) errorMedio ## [1] 0.001437009 Gráfica del valor predicho y el valor real par(mfrow=c(2,1)) plot(salidaMerval$salida * maximo, main = &quot;Valores Predichos&quot;) plot(datos_y * maximo, main = &quot;Valores Reales&quot;) Predecimos un nuevo valor Tomamos los últimos 5 valores del dataset y predecimos cual será el próximo valor. ultimosDatos &lt;- (merval[seq(nrow(merval)-4,nrow(merval)),] / maximo) %&gt;% as.matrix() ultimosDatos &lt;- t(ultimosDatos) ultimosDatos &lt;- rbind(ultimosDatos,ultimosDatos) %&gt;% as.matrix() #usamos dos registros por el tipo de datos. salidaUno &lt;- as.matrix(c(1,1)) salidaMervalNew &lt;- aplicarRedRBF(modeloMerval, ultimosDatos, salidaUno) #Nuevo valor predicho salidaMervalNew$salida[1] * maximo ## X1 ## 851.0265 # Guardamos los modelos generados if (calcular) { save(modeloMerval, modeloMerval70,file = &quot;resultadosG2.RData&quot;) } 2.3 Ejercicio 3 Lectura de datos circulo &lt;- read_csv(&quot;../../PUBLICO/Encuentro 3/Práctica/data/circulo.csv&quot;, col_names = FALSE) te &lt;- read_csv(&quot;../../PUBLICO/Encuentro 3/Práctica/data/te.csv&quot;, col_names = FALSE) Graficamos los datos de entrada par(mfrow=c(1,2)) plot(circulo) plot(te) Inicialización de Grilla SOM Implementamos una función con dos casos, una red cuadrada y una red lineal. # Definimos la cantidad de nodos. # El parámetro es la cantidad de nodos a lo ancho/largo de la grilla inicializarGrilla &lt;- function(cantidadNodos, forma = &quot;cuadrada&quot;) { if(forma == &quot;cuadrada&quot;) { # Defino conexiones entre puntos g &lt;- make_lattice( c(cantidadNodos,cantidadNodos) ) # Defino la ubicación de los puntos en el gráfico # Se distribuyen en un cuadrado de 1 por 1 para trabajar con datos normalizados. miLayout &lt;- c(rep(seq(0,cantidadNodos-1),cantidadNodos)) for (i in seq(0,cantidadNodos-1)) { miLayout &lt;- c(miLayout,rep(i,cantidadNodos)) } miLayout &lt;- matrix(miLayout, ncol = 2, nrow = cantidadNodos * cantidadNodos) miLayout &lt;- ((miLayout / cantidadNodos) * 2 ) - 1 # Generamos el gráfico plot(g, layout=miLayout) } if(forma == &quot;lineal&quot;) { # Defino conexiones entre puntos g &lt;- make_lattice( c(cantidadNodos) ) # Defino la ubicación de los puntos en el gráfico # Se distribuyen en un cuadrado de 1 por 1 para trabajar con datos normalizados. miLayout &lt;- c(seq(0,cantidadNodos-1),rep(0,cantidadNodos)) miLayout &lt;- matrix(miLayout, ncol = 2, nrow = cantidadNodos) miLayout[,1] &lt;- ((miLayout / cantidadNodos) * 2 )[,1] - 1 # Generamos el gráfico plot(g, layout=miLayout) } return(list(miLayout = miLayout, g = g)) } cantidadNodos &lt;- 5 grillaSOM &lt;- inicializarGrilla(cantidadNodos, forma = &quot;cuadrada&quot;) g &lt;- grillaSOM$g miLayout &lt;- grillaSOM$miLayout Función de vecindad Implementamos una función vecindad. Devuelve los ID de los vecinos de un nodo en un entorno cuadrado. vecindad &lt;- function(nodo, g, entorno = 1) { vecinos &lt;- which(g[nodo]==1) if(entorno &gt; 1) { for (n in seq(1,entorno-1)) { vecinosAux &lt;- which(apply(g[vecinos] %&gt;% as.matrix(), 2, sum) != 0) vecinos &lt;- unique(c(vecinos,vecinosAux)) } } vecinos = unique(c(vecinos,nodo)) return(vecinos) } #vecindad(nodo = 45, g, entorno = 2) Entrenamiento de la res SOM - Circulo Implementamos la función de entrenamiento para redes SOM, y se la utiliza en el caso del Circulo. La función dentro tiene tres etapas, - Ordenamiento topológico o global - Transición - Ajuste fino La vecindad para el ajuste de los pesos decrece en forma lineal hasta 1 y la taza de aprendizaje decrece linealmente hasta 0,05. entrenemientoSOM &lt;- function(datos, nu = 0.9, cantidadEpocas = 100, g, miLayout, cantidadNodos, entorno, criterioSalida = 0.001) { cantidadDatos &lt;- nrow(datos) graficas &lt;- list() # 1) Ordenamiento topológico o global for (e in seq(1,cantidadEpocas)) { #Hacemos un for para recorrer todos los datos de entrada miLayoutAux &lt;- miLayout for (i in seq(1,cantidadDatos)) { # para cada valor recorremos la red SOM y buscamos la distancia menor aux_1 &lt;- (datos[i,1] %&gt;% as.numeric()) - miLayout[,1] aux_2 &lt;- (datos[i,2] %&gt;% as.numeric()) - miLayout[,2] aux_1 &lt;- (aux_1)^2 aux_2 &lt;- (aux_2)^2 aux2 &lt;- sqrt( aux_1 + aux_2 ) distMenor &lt;- min(aux2) nodoGanador &lt;- which(aux2 == min(aux2))[1] #ajustar los pesos de la neurona ganadora y las neuronas vecinas # 1) Ordenamiento topológico o global if(e &lt; cantidadEpocas/15) { vecinos &lt;- vecindad(nodo = nodoGanador, g, entorno = entorno) for (k in vecinos) { miLayout[k,] &lt;- (miLayout[k,] + nu * (datos[i,] - miLayout[k,])) %&gt;% as.numeric() } } else { # 2) Transicion if(e &lt; (10*cantidadEpocas/15)) { vecinos &lt;- vecindad(nodo = nodoGanador, g, entorno = entorno) for (k in vecinos) { miLayout[k,] &lt;- (miLayout[k,] + nu * (datos[i,] - miLayout[k,])) %&gt;% as.numeric() } entorno &lt;- max(entorno - 1, 1) nu &lt;- max(nu - 0.05, 0.05) } else { # 3) Ajuste fino nu &lt;- 0.05 entorno &lt;- 1 vecinos &lt;- vecindad(nodo = nodoGanador, g, entorno = entorno) for (k in vecinos) { miLayout[k,] &lt;- (miLayout[k,] + nu * (datos[i,] - miLayout[k,])) %&gt;% as.numeric() } } } } #if(((e%%5)==0)|e==1|e==2) { #hacemos un dibujo cada 5 épocas if(TRUE) { #hacemos un dibujo cada época plot(g, layout=miLayout) print(&quot;Epoca&quot;) print(e) graficas[[e]] &lt;- list(g=g,miLayout=miLayout) } # Controlo si hay diferencias significativas con la época anterior. # sinDiferencia &lt;- TRUE # for (j in seq(1,cantidadNodos * cantidadNodos)) { # # buscar menor distancia # aux &lt;- (miLayoutAux[j,] - miLayout[j,])^2 # aux2 &lt;- sqrt( aux[1] + aux[2] ) # if(aux2 &gt; criterioSalida) { # sinDiferencia &lt;- FALSE # break # } # } # if(sinDiferencia) { # plot(g, layout=miLayout) # print(&quot;Epoca&quot;) # print(e) # graficas[[e]] &lt;- recordPlot() # return(list(graficas = graficas, layout=miLayout, g=g, epocas = e)) # } } return(list(graficas = graficas, milayout=miLayout, g=g, epocas = e)) } datos &lt;- circulo nu &lt;- 0.4 cantidadEpocas &lt;- 30 entorno &lt;- 3 if (calcular) { modeloCirculo &lt;- entrenemientoSOM(datos, nu, cantidadEpocas, g, miLayout, cantidadNodos, entorno) } # Graficas: par(mfrow=c(1,2)) plot(modeloCirculo$graficas[[1]]$g, layout=modeloCirculo$graficas[[1]]$miLayout) plot(modeloCirculo$graficas[[2]]$g, layout=modeloCirculo$graficas[[2]]$miLayout) par(mfrow=c(1,2)) plot(modeloCirculo$graficas[[3]]$g, layout=modeloCirculo$graficas[[3]]$miLayout) plot(modeloCirculo$graficas[[5]]$g, layout=modeloCirculo$graficas[[5]]$miLayout) par(mfrow=c(1,2)) plot(modeloCirculo$graficas[[10]]$g, layout=modeloCirculo$graficas[[10]]$miLayout) plot(modeloCirculo$graficas[[15]]$g, layout=modeloCirculo$graficas[[15]]$miLayout) par(mfrow=c(1,2)) plot(modeloCirculo$graficas[[20]]$g, layout=modeloCirculo$graficas[[20]]$miLayout) plot(modeloCirculo$graficas[[25]]$g, layout=modeloCirculo$graficas[[25]]$miLayout) par(mfrow=c(1,2)) plot(circulo) plot(modeloCirculo$graficas[[modeloCirculo$epocas]]$g, layout=modeloCirculo$graficas[[modeloCirculo$epocas]]$miLayout) Inicialización de Grilla SOM - Te cantidadNodos &lt;- 5 grillaSOM &lt;- inicializarGrilla(cantidadNodos, forma = &quot;cuadrada&quot;) g &lt;- grillaSOM$g miLayout &lt;- grillaSOM$miLayout Entrenamiento de la res SOM - Te datos &lt;- te nu &lt;- 0.4 cantidadEpocas &lt;- 30 entorno &lt;- 3 if (calcular) { modeloTe &lt;- entrenemientoSOM(datos, nu, cantidadEpocas, g, miLayout, cantidadNodos, entorno) } # Graficas: par(mfrow=c(1,2)) plot(modeloTe$graficas[[1]]$g, layout=modeloTe$graficas[[1]]$miLayout) plot(modeloTe$graficas[[2]]$g, layout=modeloTe$graficas[[2]]$miLayout) par(mfrow=c(1,2)) plot(modeloTe$graficas[[3]]$g, layout=modeloTe$graficas[[3]]$miLayout) plot(modeloTe$graficas[[5]]$g, layout=modeloTe$graficas[[5]]$miLayout) par(mfrow=c(1,2)) plot(modeloTe$graficas[[10]]$g, layout=modeloTe$graficas[[10]]$miLayout) plot(modeloTe$graficas[[15]]$g, layout=modeloTe$graficas[[15]]$miLayout) par(mfrow=c(1,2)) plot(modeloTe$graficas[[20]]$g, layout=modeloTe$graficas[[20]]$miLayout) plot(modeloTe$graficas[[25]]$g, layout=modeloTe$graficas[[25]]$miLayout) par(mfrow=c(1,2)) plot(te) plot(modeloTe$graficas[[modeloTe$epocas]]$g, layout=modeloTe$graficas[[modeloTe$epocas]]$miLayout) Inicialización de Grilla SOM Unidimensional - Te cantidadNodos &lt;- 25 grillaSOM &lt;- inicializarGrilla(cantidadNodos, forma = &quot;lineal&quot;) g &lt;- grillaSOM$g miLayout &lt;- grillaSOM$miLayout Entrenamiento de la res SOM Unidimensional - Te datos &lt;- te nu &lt;- 0.4 cantidadEpocas &lt;- 30 cantidadNodos &lt;- 5 entorno &lt;- 7 if (calcular) { modeloTeUni &lt;- entrenemientoSOM(datos, nu, cantidadEpocas, g, miLayout, cantidadNodos, entorno) } # Graficas: par(mfrow=c(1,2)) plot(modeloTeUni$graficas[[1]]$g, layout=modeloTeUni$graficas[[1]]$miLayout) plot(modeloTeUni$graficas[[2]]$g, layout=modeloTeUni$graficas[[2]]$miLayout) par(mfrow=c(1,2)) plot(modeloTeUni$graficas[[3]]$g, layout=modeloTeUni$graficas[[3]]$miLayout) plot(modeloTeUni$graficas[[5]]$g, layout=modeloTeUni$graficas[[5]]$miLayout) par(mfrow=c(1,2)) plot(modeloTeUni$graficas[[10]]$g, layout=modeloTeUni$graficas[[10]]$miLayout) plot(modeloTeUni$graficas[[15]]$g, layout=modeloTeUni$graficas[[15]]$miLayout) par(mfrow=c(1,2)) plot(modeloTeUni$graficas[[20]]$g, layout=modeloTeUni$graficas[[20]]$miLayout) plot(modeloTeUni$graficas[[25]]$g, layout=modeloTeUni$graficas[[25]]$miLayout) par(mfrow=c(1,2)) plot(te) plot(modeloTeUni$graficas[[modeloTeUni$epocas]]$g, layout=modeloTeUni$graficas[[modeloTeUni$epocas]]$miLayout) Podemos observar que con la misma cantidad de neuronas, pero con la distribución lineal podemos obtener una red que se ubica por completo sobre los datos con el mismo entrenamiento. 2.4 Ejercicio 4 Lectura de datos clouds &lt;- read_csv(&quot;../../PUBLICO/Encuentro 3/Práctica/data/clouds.csv&quot;, col_names = FALSE) Graficamos los datos de entrada ggplot(data=clouds, aes(x=X1, y=X2,color=as.factor(X3)))+geom_point() Inicialización de Grilla SOM - Clouds Generamos una grilla SOM de 49 nodos, con esto tenemos un nodo cada 100 patrones aproximadamente. cantidadNodos &lt;- 7 grillaSOM &lt;- inicializarGrilla(cantidadNodos, forma = &quot;cuadrada&quot;) g &lt;- grillaSOM$g miLayout &lt;- grillaSOM$miLayout Entrenamiento de la res SOM - Clouds datos &lt;- clouds[,c(1,2)] nu &lt;- 0.4 cantidadEpocas &lt;- 30 cantidadNodos &lt;- 7 entorno &lt;- 3 if (calcular) { modeloClouds &lt;- entrenemientoSOM(datos, nu, cantidadEpocas, g, miLayout, cantidadNodos, entorno) } # Graficas: par(mfrow=c(1,2)) plot(modeloClouds$graficas[[1]]$g, layout=modeloClouds$graficas[[1]]$miLayout) plot(modeloClouds$graficas[[2]]$g, layout=modeloClouds$graficas[[2]]$miLayout) par(mfrow=c(1,2)) plot(modeloClouds$graficas[[3]]$g, layout=modeloClouds$graficas[[3]]$miLayout) plot(modeloClouds$graficas[[5]]$g, layout=modeloClouds$graficas[[5]]$miLayout) par(mfrow=c(1,2)) plot(modeloClouds$graficas[[10]]$g, layout=modeloClouds$graficas[[10]]$miLayout) plot(modeloClouds$graficas[[15]]$g, layout=modeloClouds$graficas[[15]]$miLayout) par(mfrow=c(1,2)) plot(modeloClouds$graficas[[20]]$g, layout=modeloClouds$graficas[[20]]$miLayout) plot(modeloClouds$graficas[[25]]$g, layout=modeloClouds$graficas[[25]]$miLayout) par(mfrow=c(1,2)) plot(clouds[,c(1,2)]) plot(modeloClouds$graficas[[modeloClouds$epocas]]$g, layout=modeloClouds$graficas[[modeloClouds$epocas]]$miLayout) Etiquetado de neuronas Para el etiquetado de neuronas, se evalúa la cantidad de patrones de cada clase en el entorno cercano de cada neurona. Se toma como entorno cercano un radio de la mitad de la distancia promedio de la neurona con sus vecinas. clase &lt;- 3 # defino columna de clase claseResultadoRed &lt;- array(0) miLayout &lt;- modeloClouds$milayout g &lt;- modeloClouds$g for (i in seq(1,cantidadNodos*cantidadNodos)) { # calculo la distancia media a sus vecinos para definir un entorno vecinos &lt;- which(g[i]==1) aux_1 &lt;- miLayout[vecinos,1] - miLayout[i,1] aux_2 &lt;- miLayout[vecinos,2] - miLayout[i,2] aux_1 &lt;- (aux_1)^2 aux_2 &lt;- (aux_2)^2 aux2 &lt;- sqrt( aux_1 + aux_2 ) radio &lt;- mean(aux2) / 2 # se clasifica el nodo con la categoría que más se repita en el entorno acumuladores &lt;- array(0) aux_1 &lt;- datos[,1] - miLayout[i,1] aux_2 &lt;- datos[,2] - miLayout[i,2] aux_1 &lt;- (aux_1)^2 aux_2 &lt;- (aux_2)^2 aux2 &lt;- sqrt( aux_1 + aux_2 ) if(length(which(clouds[which(aux2 &lt; radio),clase]==0)) &gt; length(which(clouds[which(aux2 &lt; radio),clase]==1))) { claseResultadoRed[i] &lt;- 0 } else { claseResultadoRed[i] &lt;- 1 } } Clasificación con red SOM Se asigna como clase ganadora a cada patrón a la clase de la neurona más cercana. #Ejemplo con un nuevo patrón nuevoDato &lt;- c(1,1) #para el nuevo valor recorremos la red SOM y buscamos la distancia menor aux &lt;- (nuevoDato - miLayout[1,])^2 distMenor &lt;- sqrt( aux[1] + aux[2] ) nodoGanador &lt;- 1 for (j in seq(2,cantidadNodos)) { # buscar menor distancia aux &lt;- (nuevoDato - miLayout[j,])^2 aux2 &lt;- sqrt( aux[1] + aux[2] ) if(aux2 &lt; distMenor) { distMenor &lt;- aux2 nodoGanador &lt;- j } } #clase del dato nuevo claseResultadoRed[nodoGanador] ## [1] 0 # Realizamos la clasificación para todos los patrones del dataset claseResultado &lt;- array(0) for (i in seq(1,nrow(datos))) { aux_1 &lt;- miLayout[,1] - (datos[i,1] %&gt;% as.numeric()) aux_2 &lt;- miLayout[,2] - (datos[i,2] %&gt;% as.numeric()) aux_1 &lt;- (aux_1)^2 aux_2 &lt;- (aux_2)^2 distancias &lt;- sqrt( aux_1 + aux_2 ) claseResultado[i] &lt;- claseResultadoRed[which(distancias == min(distancias))[1]] } Visualizamos los puntos clasificados ggplot(data=clouds, aes(x=X1, y=X2,color=as.factor(X3), shape=as.factor(claseResultado)))+geom_point() Calculamos la tasa de aciertos aciertos &lt;- length(which(clouds[,clase] == claseResultado)) tasa &lt;- aciertos / nrow(clouds) tasa ## [1] 0.86 Obtuvimos una tasa de un 86% de acierto, para mejorar esto se podría entrenar una red SOM con mayor numeró de neuronas. # Guardamos los modelos generados if (calcular) { save(modeloMerval, modeloMerval70, modeloCirculo, modeloClouds, modeloTe, modeloTeUni, file = &quot;resultadosG2.RData&quot;) } "],
["guia-3.html", "Guía 3 Guia 3 3.1 Ejercicio 1 3.2 Ejercicio 2", " Guía 3 Guia 3 3.1 Ejercicio 1 Implemente las estructuras de datos y algoritmos básicos para la solución de un problema mediante algoritmos genéticos. Pruebe estas rutinas para buscar el mínimo global de las siguientes funciones: \\[-x \\sin(\\sqrt{|x|})\\] \\[x+\\sin(3x)+8\\cos(5x)\\] \\[(x^2+y^2)^{0.25}*[\\sin^2(50*(x^2+y^2)^{0.1})+1]\\] 3.1.1 Preguntas ¿Corresponde al mínimo global el valor encontrado? Repita la búsqueda varias veces y determine el valor medio y desvío. ¿Se encuentra ahora el mínimo global dentro del intervalo? repeticionesAlgoritmo &lt;- 10 3.1.2 Función 1 # Definición de funciones a minimizar funcion01 &lt;- function(x){ return(-x*sin(sqrt(abs(x)))) } # Rango de la variable xMin &lt;- -512 xMax &lt;- 512 resultado &lt;- numeric(0) for(i in 1:repeticionesAlgoritmo) { resultadoAlgoritmo &lt;- algoritmoGenetico(cantidadIndividuos = 10, limiteInf = xMin, limiteSup = xMax, generacionesSinCambio = 40, fitnessFn = function(x) {-funcion01(x)}) resultado &lt;- c(resultado, resultadoAlgoritmo$mejor_individuo) } ## mejor individuo proceso: 420.988983154297 ## mejor individuo proceso: 421.018720028493 ## mejor individuo proceso: 420.99030280668 ## mejor individuo proceso: 420.986333993234 ## mejor individuo proceso: 420.988463429698 ## mejor individuo proceso: 420.741282582927 ## mejor individuo proceso: 421.447726715519 ## mejor individuo proceso: 421.036895751953 ## mejor individuo proceso: 420.705217043495 ## mejor individuo proceso: 421.14256811142 x &lt;- seq(-512, 512, length.out = 255) y &lt;- funcion01(x) datos &lt;- as.data.frame(cbind(x,y)) ggplot(data = datos, aes(x = x, y = y)) + geom_line() + labs(title = &quot;funcion01&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + geom_vline(xintercept = mean(resultado), color=&quot;red&quot;) Se puede apreciar en la gráfica que el mínimo encontrado coincide con el mínimo global dentro del rango. El valor medio de los resultados de la funcion01 es 421.0046494, el desvio estándar es 0.204572 3.1.3 Función 2 funcion02 &lt;- function(x){ return(x+sin(3*x)+8*cos(5*x)) } xMin &lt;- 0 xMax &lt;- 20 resultado &lt;- numeric(0) for(i in 1:repeticionesAlgoritmo) { resultadoAlgoritmo &lt;- algoritmoGenetico(cantidadIndividuos = 10, limiteInf = xMin, limiteSup = xMax, generacionesSinCambio = 40, fitnessFn = function(x) {-funcion02(x)}) resultado &lt;- c(resultado, resultadoAlgoritmo$mejor_individuo) } ## mejor individuo proceso: 1.87062095282693 ## mejor individuo proceso: 1.86787152662873 ## mejor individuo proceso: 1.87336347997189 ## mejor individuo proceso: 1.87228606082499 ## mejor individuo proceso: 1.86591755284618 ## mejor individuo proceso: 1.86577785357186 ## mejor individuo proceso: 1.87073779011412 ## mejor individuo proceso: 1.85048342244472 ## mejor individuo proceso: 1.85796645469964 ## mejor individuo proceso: 1.86310725297889 x &lt;- seq(xMin, xMax, length.out = 255) y &lt;- funcion02(x) datos &lt;- as.data.frame(cbind(x,y)) ggplot(data = datos, aes(x = x, y = y)) + geom_line() + labs(title = &quot;funcion02&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + geom_vline(xintercept = mean(resultado), color=&quot;red&quot;) Se puede apreciar en la gráfica que el mínimo encontrado coincide con el mínimo global dentro del rango. El valor medio de los resultados de la funcion01 es 1.8658132, el desvio estándar es 0.0071057 3.1.4 Función 3 funcion03 &lt;- function(x, y){ return((x^2+y^2)^0.25*(sin(50*(x^2+y^2)^0.1)^2+1)) } xMin &lt;- -100 xMax &lt;- 100 resultado &lt;- data.frame(x=numeric(), y=numeric()) for(i in 1:repeticionesAlgoritmo) { resultadoAlgoritmo &lt;- algoritmoGenetico(cantidadIndividuos = 10, cantidadVariables=2, limiteInf = c(xMin, xMin), limiteSup = c(xMax, xMax), generacionesSinCambio = 40, fitnessFn = function(x,y) {-funcion03(x,y)}) resultado[i, ] &lt;- resultadoAlgoritmo$mejor_individuo } ## mejor individuo proceso: 0.0163174411719559, 0.0163174411719559 ## mejor individuo proceso: 0.00482798837470311, 0.00482798837470311 ## mejor individuo proceso: 0.0234687051334006, 0.0234687051334006 ## mejor individuo proceso: 0.0417018375459013, 0.0417018375459013 ## mejor individuo proceso: 0.173999881371856, 0.173999881371856 ## mejor individuo proceso: 0.0418096225005214, 0.0418096225005214 ## mejor individuo proceso: 0.00535801831561319, 0.00535801831561319 ## mejor individuo proceso: 0.00223799508917632, 0.00223799508917632 ## mejor individuo proceso: 0.0119998037084161, 0.0119998037084161 ## mejor individuo proceso: -0.110787611526009, -0.110787611526009 x &lt;- y &lt;- seq(xMin, xMax, length.out = 255) z &lt;- outer(x, y, funcion03) nbcol = 100 color = rev(rainbow(nbcol, start = 0/6, end = 4/6)) zcol = cut(z, nbcol) p &lt;- plot_ly(x = x, y = y, z = z) %&gt;% add_surface( contours = list( z = list( show=TRUE, usecolormap=TRUE, highlightcolor=&quot;#ff0000&quot;, project=list(z=TRUE) ) ) ) %&gt;% layout( scene = list( camera=list( eye = list(x=1.87, y=0.88, z=-0.64) ) ) ) p resultado %&gt;% colMeans() ## x y ## 0.02109337 0.02109337 Se puede apreciar en la gráfica que el mínimo encontrado coincide con el mínimo global dentro del ranggo (tanto para x como para y). El valor medio de los resultados de la funcion01 es 0.0210934, 0.0210934, el desvio estándar es 0.0688038, 0.0688038 3.2 Ejercicio 2 En el archivo desconocido1.csv se ha registrado información de un proceso que puede describirse mediante la ecuación: \\[y1 = a_1x^3_1 + a_2 x^2_1 + a_3 x_1 + a_4\\] Se sabe que las mediciones contienen ruido, y que los parámetro del sistema se encuentran acotados en el intervalo [−5,1.5]. Utilice un algoritmo genético para determinar los parámetros delmodelo. Calcule el error cuadrático total obtenido de la comparción entre los datos provistos y la función aproximada mediante el algoritmo. ¿Qué puede concluir del ajuste? desconocido1 &lt;- read_csv(&quot;../../PUBLICO/Encuentro 5/Práctica/desconocido1.csv&quot;, col_names = FALSE) errorEj2 &lt;- function(a1, a2, a3, a4){ x &lt;- desconocido1[, 1] %&gt;% as.matrix(ncol=1) %&gt;% t() y &lt;- matrix(rep(desconocido1[, 2], length(a1)) %&gt;% unlist(), ncol=length(x)) # habrá 128 salidas por individuo (una por cada dato del dataset) # hay tantos individuos como se configure en el algoritmo (100 en este caso) salidaEstimada &lt;- a1 %*% x^3 + a2 %*% x^2 + a3 %*% x + a4 # la matriz de error es de 100x128 error &lt;- y-salidaEstimada # los renglones representan a los individuos evaluados error_cuadratico_medio = rowMeans(error^2) return(error_cuadratico_medio) } # Rango de los parametros xMin &lt;- -5 xMax &lt;- 1.5 resultado &lt;- algoritmoGenetico(limiteInf = rep(xMin,4), limiteSup = rep(xMax,4), cantidadVariables = 4, fitnessFn = function(a1, a2, a3, a4) {-errorEj2(a1, a2, a3, a4)}) ## mejor individuo proceso: 0.963261821190827, -2.28645942430012, -0.872262658085674, -2.70829578477424 "]
]
