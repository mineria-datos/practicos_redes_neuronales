---
title: "Guia 1, ejercicio 4"
author: "Grupo 3"
date: "1/16/2020"
output: html_document
---


```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(plotly)
library(foreach)

## ejecuciones multicore
#install.packages("doMC", repos="http://R-Forge.R-project.org")
library(doMC)
registerDoMC()
getDoParWorkers() 
source('./PerceptronSimple.R')
source('./PerceptronMulticapa.R')
source('./Particiones.R')
source('./LeavePOut.R')
calcular = TRUE
 if(file.exists('./resultados_ejercicio4.RData')) {
  load("./resultados_ejercicio4.RData")
  calcular = FALSE
}

```

## Ejercicio 4


> En ejercicio 4 quedó un head(30) para disminuir la cantidad de combinaciones de leave2out


 - Lectura de los patrones de entrenamiento

```{r message=FALSE}
irisbin <- read_csv("../../PUBLICO/Encuentro 1/Práctica/data/irisbin.csv", col_names = FALSE) %>% head(30)

```

- Selección de parámetros y entrenamiento de perceptrón
Utilizamos las funciones implementadas en el archivo "PerceptronMulticapa.R"


### Leave One Out
```{r}
if(calcular) {
  system.time({
    errores_leave_one_out <- foreach(e = leavePOutSplit(irisbin, 1), .combine = c)  %dopar% { #para ejecucion secuencia %do% {
      y_real <- e$test[5:7] %>% as.matrix()
      modelo <- entrenarPerceptronM(e$train, maxEpocas = 100, critFinalizacion = 0.8, arquitectura = c(4,3,3))
      y_predicha <- modelo$predecir(e$test)
      e <- y_real - y_predicha
      error <- e %*% t(e)
      error
    }
  })
}
```
La media de los errores leave_one_out es: `r mean(errores_leave_one_out)`
El desvio standard de los errores leave_one_out es: `r sd(errores_leave_one_out)`

```{r fig.cap="grafica del error"}
errores_leave_one_out %>%
  data.frame(x=.) %>% 
  ggplot(aes(x=x)) +
  geom_density() +
  ggtitle("Densidad del error", subtitle = "leave 1 out")
    
```


### Leave 2 Out
```{r}
if(calcular) {
  system.time({
    split <- leavePOutSplit(irisbin, 2)
    errores_leave_2_out <- foreach(e = split, .combine = c)  %dopar% { #para ejecucion secuencia %do% {
      y_real <- e$test[5:7] %>% as.matrix()
      modelo <- entrenarPerceptronM(e$train, maxEpocas = 100, critFinalizacion = 0.7, arquitectura = c(4,3,3))
      print(glue::glue("progreso: {e$actual}/{e$nroCombinaciones}"))
      y_predicha <- modelo$predecir(e$test)
      e <- y_real - y_predicha
      error <- c(e[1,] %*% e[1,], e[2,] %*% e[2,])
      error
    }
  })

}
```
La media de los errores leave_one_out es: `r mean(errores_leave_one_out)`
El desvio standard de los errores leave_one_out es: `r sd(errores_leave_one_out)`

```{r}
errores_leave_2_out %>%
  data.frame(x=.) %>% 
  ggplot(aes(x=x)) +
  geom_density() +
  ggtitle("Densidad del error", subtitle = "leave 2 out")
```


```{r}

# Guardamos los resultados calculados
if (calcular) {
  save(errores_leave_one_out, errores_leave_2_out, file = "resultados_ejercicio4.RData")
}

```


### Respuestas

¿Cuál es la arquitectura mı́nima que emplearı́a para resolver
este problema? Justifique.

La arquitectura mínima requerida para resolver este problema es una de 3 capas, sindo las capas de entrada y salida determinadas en su número por la formulación del problema (4 neuronas en la capa de entrada y 3 en la de salida).

Una arquitectura de 3 capas permite separar regiones de decisión de arbitraria complejidad, requirida por la distribución de los datos como se aprecia en la siguiente figura:

```{r}
irisbin %>% 
  mutate(clase = case_when(
    X5 == -1 & X6 == -1 & X7 == 1 ~ "setosa",
    X5 == -1 & X6 == 1 & X7 == -1 ~ "versicolor",
    X5 == 1 & X6 == -1 & X7 == -1 ~ "virginica",
  )) %>% 
  ggplot(aes(x=X1, y=X2, color=clase)) +
  geom_point()
```




¿Qué ventajas y desventajas poseen los métodos leave-k-out
y leave-one-out para validación?


La principal ventaja es que son métodos exhaustivos, por lo que usan todos los datos disponibles, resultando de gran utilidad cuando los mismos son escasos.

La principal desventaja es el costo computacional de entrenar tantas veces el modelo, para $n$ el número de datos y $k$ el número de observaciones que se usan en cada test, el número de ciclos de entrenamiento/test es la combinatoria $(_p^n)$.




